{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ben/VSCode/banditLiquidDem/.conda/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from exp_framework.Ensemble import Ensemble, PretrainedEnsemble, StudentExpertEnsemble\n",
    "from exp_framework.delegation import (\n",
    "    DelegationMechanism,\n",
    "    UCBDelegationMechanism,\n",
    "    ProbaSlopeDelegationMechanism,\n",
    "    RestrictedMaxGurusDelegationMechanism,\n",
    "    StudentExpertDelegationMechanism,\n",
    ")\n",
    "from exp_framework.learning import Net\n",
    "from exp_framework.experiment import (\n",
    "    Experiment,\n",
    "    calculate_avg_std_test_accs,\n",
    "    calculate_avg_std_train_accs,\n",
    ")\n",
    "from avalanche.training.supervised import Naive\n",
    "from matplotlib import pyplot as plt\n",
    "from exp_framework.data_utils import Data\n",
    "from avalanche.benchmarks.classic import RotatedMNIST, SplitMNIST\n",
    "import numpy as np\n",
    "import matplotlib as mpl\n",
    "import seaborn as sns\n",
    "from itertools import product\n",
    "import pandas as pd\n",
    "import torch.optim as optim\n",
    "from torch.nn import CrossEntropyLoss\n",
    "\n",
    "from avalanche.training.plugins import (\n",
    "    CWRStarPlugin,\n",
    "    ReplayPlugin,\n",
    "    EWCPlugin,\n",
    "    TrainGeneratorAfterExpPlugin,\n",
    "    LwFPlugin,\n",
    "    SynapticIntelligencePlugin,\n",
    ")\n",
    "from exp_framework.MinibatchEvalAccuracy import MinibatchEvalAccuracy\n",
    "from avalanche.training.plugins import EvaluationPlugin\n",
    "from avalanche.evaluation.metrics import accuracy_metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### learning the mapping $\\mathcal{X} \\rightarrow \\mathcal{G}$ (i.e. $\\mathcal{X} \\rightarrow \\mathcal{Y}\\times\\mathcal{C}$)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "window_size = 50\n",
    "num_trials = 2\n",
    "n_voters = 5\n",
    "\n",
    "\n",
    "# # Set up the Class Incremental framework\n",
    "# data = Data(\n",
    "#     data_set_name=\"mnist\",\n",
    "#     # train_digit_groups=[range(5), range(5, 10)],\n",
    "#     # train_digit_groups=[[0, 1, 2], [3, 4, 5,], [6, 7, 8, 9]],\n",
    "#     train_digit_groups=[[0, 1], [2, 3], [4, 5], [6, 7], [8, 9]],\n",
    "#     # test_digit_groups=[[0, 1], [2, 3], [4, 5], [6, 7], [8, 9]],\n",
    "#     # test_digit_groups=[range(5), range(5, 10)],\n",
    "#     test_digit_groups=[range(10)],\n",
    "#     batch_size=batch_size,\n",
    "# )\n",
    "\n",
    "data = SplitMNIST(n_experiences=5, fixed_class_order=list(range(10)))\n",
    "# if data == \"MNIST\":\n",
    "#     benchmark = SplitMNIST(n_experiences=5, fixed_class_order=list(range(10)), seed=self.seed)\n",
    "# elif data == \"RotatedMNIST\":\n",
    "#     benchmark = RotatedMNIST(n_experiences=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Single Active Voter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Delegation Mechanisms and Ensembles\n",
    "\n",
    "For simplicity, only explore full ensemble and variants of ProbaSlopeDelegationMechanism since they can be created programmatically."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create Delegation Mechanisms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Delegation Mechanisms - single guru\n",
    "\n",
    "NOOP_del_mech = DelegationMechanism(batch_size=batch_size, window_size=window_size)\n",
    "\n",
    "probability_functions = [\n",
    "    \"random_better\",\n",
    "    \"probabilistic_better\",\n",
    "    \"probabilistic_weighted\",\n",
    "]\n",
    "score_functions = [\n",
    "    \"accuracy_score\",\n",
    "    \"balanced_accuracy_score\",\n",
    "    \"f1_score\",\n",
    "    \"precision_score\",\n",
    "    \"recall_score\",\n",
    "    \"top_k_accuracy_score\",\n",
    "    \"roc_auc_score\",\n",
    "    \"log_loss_score\",\n",
    "    \"max_diversity\",\n",
    "]\n",
    "probability_functions = [\"max_diversity\"]\n",
    "score_functions = [\"accuracy_score\"]\n",
    "max_active_gurus = 1\n",
    "\n",
    "del_mechs = {\"full-ensemble\": NOOP_del_mech}\n",
    "for prob_func, score_func in product(probability_functions, score_functions):\n",
    "    dm = ProbaSlopeDelegationMechanism(\n",
    "        batch_size=batch_size,\n",
    "        window_size=window_size,\n",
    "        max_active=max_active_gurus,\n",
    "        probability_function=prob_func,\n",
    "        score_method=score_func,\n",
    "    )\n",
    "    del_mechs[f\"{prob_func}-{score_func}\"] = dm\n",
    "\n",
    "\n",
    "ensembles_dict = {\n",
    "    dm_name: Ensemble(\n",
    "        training_epochs=1,\n",
    "        n_voters=n_voters,\n",
    "        delegation_mechanism=dm,\n",
    "        name=dm_name,\n",
    "        input_dim=28 * 28,\n",
    "        output_dim=10,\n",
    "    )\n",
    "    for dm_name, dm in del_mechs.items()\n",
    "}\n",
    "\n",
    "# restricted_max_gurus_mech = RestrictedMaxGurusDelegationMechanism(\n",
    "#     batch_size=batch_size,\n",
    "#     num_voters=n_voters,\n",
    "#     max_active_voters=max_active_gurus,\n",
    "#     window_size=window_size,\n",
    "#     t_between_delegation=3,\n",
    "# )\n",
    "# UCB_del_mech = UCBDelegationMechanism(\n",
    "#     batch_size=batch_size,\n",
    "#     window_size=window_size,\n",
    "#     ucb_window_size=None\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create Avalanche Strategies to Compare Against"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_strategies_to_evaluate():\n",
    "\n",
    "    model = Net(input_dim=28 * 28, output_dim=10)\n",
    "    # model = SimpleMLP(num_classes=10)\n",
    "    optimize = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "    plugins_to_evaluate = {\n",
    "        \"LwF\": LwFPlugin(),\n",
    "        # \"EWC\": EWCPlugin(ewc_lambda=0.001),\n",
    "        # \"SynapticIntelligence\": SynapticIntelligencePlugin(si_lambda=0.5),\n",
    "        # \"Replay\": ReplayPlugin(mem_size=100),\n",
    "    }\n",
    "\n",
    "    strategies_to_evaluate = {}\n",
    "    for name, pte in plugins_to_evaluate.items():\n",
    "        mb_eval = MinibatchEvalAccuracy()\n",
    "        evp = EvaluationPlugin(\n",
    "            accuracy_metrics(minibatch=True, epoch=True, experience=True, stream=True),\n",
    "            mb_eval\n",
    "        )\n",
    "        cl_strategy = Naive(\n",
    "            model=model,\n",
    "            optimizer=optimize,\n",
    "            criterion=CrossEntropyLoss(),\n",
    "            train_mb_size=batch_size,\n",
    "            train_epochs=1,\n",
    "            eval_mb_size=batch_size,\n",
    "            plugins=[pte, evp, mb_eval],\n",
    "            # evaluator=evp,\n",
    "        )\n",
    "        strategies_to_evaluate[name] = (cl_strategy, evp)\n",
    "    \n",
    "    return strategies_to_evaluate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Ensemble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/2 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ben/VSCode/banditLiquidDem/.conda/lib/python3.10/site-packages/avalanche/training/plugins/evaluation.py:68: UserWarning: No loggers specified, metrics will not be logged\n",
      "  warnings.warn(\"No loggers specified, metrics will not be logged\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting trial  0\n",
      "-- >> Start of training phase << --\n",
      "100%|██████████| 99/99 [00:00<00:00, 161.73it/s]\n",
      "Epoch 0 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream/Task000 = 0.0755\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream/Task000 = 0.9878\n",
      "-- >> End of training phase << --\n",
      "-- >> Start of training phase << --\n",
      "100%|██████████| 95/95 [00:00<00:00, 150.53it/s]\n",
      "Epoch 0 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream/Task000 = 1.4892\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream/Task000 = 0.8196\n",
      "-- >> End of training phase << --\n",
      "-- >> Start of training phase << --\n",
      "100%|██████████| 88/88 [00:00<00:00, 154.79it/s]\n",
      "Epoch 0 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream/Task000 = 2.0096\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream/Task000 = 0.7018\n",
      "-- >> End of training phase << --\n",
      "-- >> Start of training phase << --\n",
      "100%|██████████| 96/96 [00:00<00:00, 146.00it/s]\n",
      "Epoch 0 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream/Task000 = 2.0663\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream/Task000 = 0.7007\n",
      "-- >> End of training phase << --\n",
      "-- >> Start of training phase << --\n",
      "100%|██████████| 93/93 [00:00<00:00, 151.44it/s]\n",
      "Epoch 0 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream/Task000 = 3.2583\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream/Task000 = 0.5389\n",
      "-- >> End of training phase << --\n",
      "<avalanche.benchmarks.scenarios.new_classes.nc_scenario.NCExperience object at 0x295ff6950>\n",
      "<avalanche.benchmarks.scenarios.new_classes.nc_scenario.NCExperience object at 0x295ff5d20>\n",
      "<avalanche.benchmarks.scenarios.new_classes.nc_scenario.NCExperience object at 0x295ff6dd0>\n",
      "<avalanche.benchmarks.scenarios.new_classes.nc_scenario.NCExperience object at 0x295ff6a10>\n",
      "<avalanche.benchmarks.scenarios.new_classes.nc_scenario.NCExperience object at 0x295ff4b80>\n",
      "Experience results are: \n",
      "{'full-ensemble': [0.0, 0.0, 0.0, 0.0, 0.9741055928170681], 'max_diversity-accuracy_score': [0.0, 0.02055584010668099, 0.20040650367736818, 0.3683712128549814, 0.8841378353536129]}\n",
      "-- >> Start of eval phase << --\n",
      "-- Starting eval on experience 0 (Task 0) from test stream --\n",
      "100%|██████████| 17/17 [00:00<00:00, 198.10it/s]\n",
      "> Eval on experience 0 (Task 0) from test stream ended.\n",
      "\tLoss_Exp/eval_phase/test_stream/Task000/Exp000 = 18.2112\n",
      "\tTop1_Acc_Exp/eval_phase/test_stream/Task000/Exp000 = 0.0000\n",
      "-- Starting eval on experience 1 (Task 0) from test stream --\n",
      "100%|██████████| 16/16 [00:00<00:00, 199.83it/s]\n",
      "> Eval on experience 1 (Task 0) from test stream ended.\n",
      "\tLoss_Exp/eval_phase/test_stream/Task000/Exp001 = 18.3767\n",
      "\tTop1_Acc_Exp/eval_phase/test_stream/Task000/Exp001 = 0.0000\n",
      "-- Starting eval on experience 2 (Task 0) from test stream --\n",
      "100%|██████████| 15/15 [00:00<00:00, 201.33it/s]\n",
      "> Eval on experience 2 (Task 0) from test stream ended.\n",
      "\tLoss_Exp/eval_phase/test_stream/Task000/Exp002 = 12.7232\n",
      "\tTop1_Acc_Exp/eval_phase/test_stream/Task000/Exp002 = 0.0000\n",
      "-- Starting eval on experience 3 (Task 0) from test stream --\n",
      "100%|██████████| 16/16 [00:00<00:00, 150.60it/s]\n",
      "> Eval on experience 3 (Task 0) from test stream ended.\n",
      "\tLoss_Exp/eval_phase/test_stream/Task000/Exp003 = 6.8322\n",
      "\tTop1_Acc_Exp/eval_phase/test_stream/Task000/Exp003 = 0.0000\n",
      "-- Starting eval on experience 4 (Task 0) from test stream --\n",
      "100%|██████████| 16/16 [00:00<00:00, 202.05it/s]\n",
      "> Eval on experience 4 (Task 0) from test stream ended.\n",
      "\tLoss_Exp/eval_phase/test_stream/Task000/Exp004 = 0.1197\n",
      "\tTop1_Acc_Exp/eval_phase/test_stream/Task000/Exp004 = 0.9571\n",
      "-- >> End of eval phase << --\n",
      "\tLoss_Stream/eval_phase/test_stream/Task000 = 11.3691\n",
      "\tTop1_Acc_Stream/eval_phase/test_stream/Task000 = 0.1898\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 1/2 [00:12<00:12, 12.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting trial  1\n",
      "-- >> Start of training phase << --\n",
      "100%|██████████| 99/99 [00:00<00:00, 156.36it/s]\n",
      "Epoch 0 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream/Task000 = 0.0807\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream/Task000 = 0.9904\n",
      "-- >> End of training phase << --\n",
      "-- >> Start of training phase << --\n",
      "100%|██████████| 95/95 [00:00<00:00, 145.72it/s]\n",
      "Epoch 0 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream/Task000 = 1.4354\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream/Task000 = 0.8337\n",
      "-- >> End of training phase << --\n",
      "-- >> Start of training phase << --\n",
      "100%|██████████| 88/88 [00:00<00:00, 148.70it/s]\n",
      "Epoch 0 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream/Task000 = 1.9490\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream/Task000 = 0.6927\n",
      "-- >> End of training phase << --\n",
      "-- >> Start of training phase << --\n",
      "100%|██████████| 96/96 [00:00<00:00, 148.20it/s]\n",
      "Epoch 0 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream/Task000 = 2.1216\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream/Task000 = 0.7384\n",
      "-- >> End of training phase << --\n",
      "-- >> Start of training phase << --\n",
      "100%|██████████| 93/93 [00:00<00:00, 157.21it/s]\n",
      "Epoch 0 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream/Task000 = 2.7877\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream/Task000 = 0.6216\n",
      "-- >> End of training phase << --\n",
      "<avalanche.benchmarks.scenarios.new_classes.nc_scenario.NCExperience object at 0x295ff6f80>\n",
      "<avalanche.benchmarks.scenarios.new_classes.nc_scenario.NCExperience object at 0x295ff7250>\n",
      "<avalanche.benchmarks.scenarios.new_classes.nc_scenario.NCExperience object at 0x295ff6ef0>\n",
      "<avalanche.benchmarks.scenarios.new_classes.nc_scenario.NCExperience object at 0x295ff6fe0>\n",
      "<avalanche.benchmarks.scenarios.new_classes.nc_scenario.NCExperience object at 0x295ff4460>\n",
      "Experience results are: \n",
      "{'full-ensemble': [0.0, 0.0, 0.0, 0.0, 0.9706876240670681], 'max_diversity-accuracy_score': [0.0, 0.021996670053340495, 0.022751524299383163, 0.988340437412262, 0.02294921875]}\n",
      "-- >> Start of eval phase << --\n",
      "-- Starting eval on experience 0 (Task 0) from test stream --\n",
      "100%|██████████| 17/17 [00:00<00:00, 201.05it/s]\n",
      "> Eval on experience 0 (Task 0) from test stream ended.\n",
      "\tLoss_Exp/eval_phase/test_stream/Task000/Exp000 = 17.4604\n",
      "\tTop1_Acc_Exp/eval_phase/test_stream/Task000/Exp000 = 0.0000\n",
      "-- Starting eval on experience 1 (Task 0) from test stream --\n",
      "100%|██████████| 16/16 [00:00<00:00, 200.05it/s]\n",
      "> Eval on experience 1 (Task 0) from test stream ended.\n",
      "\tLoss_Exp/eval_phase/test_stream/Task000/Exp001 = 18.2663\n",
      "\tTop1_Acc_Exp/eval_phase/test_stream/Task000/Exp001 = 0.0000\n",
      "-- Starting eval on experience 2 (Task 0) from test stream --\n",
      "100%|██████████| 15/15 [00:00<00:00, 199.18it/s]\n",
      "> Eval on experience 2 (Task 0) from test stream ended.\n",
      "\tLoss_Exp/eval_phase/test_stream/Task000/Exp002 = 13.4881\n",
      "\tTop1_Acc_Exp/eval_phase/test_stream/Task000/Exp002 = 0.0000\n",
      "-- Starting eval on experience 3 (Task 0) from test stream --\n",
      "100%|██████████| 16/16 [00:00<00:00, 148.67it/s]\n",
      "> Eval on experience 3 (Task 0) from test stream ended.\n",
      "\tLoss_Exp/eval_phase/test_stream/Task000/Exp003 = 6.7899\n",
      "\tTop1_Acc_Exp/eval_phase/test_stream/Task000/Exp003 = 0.0000\n",
      "-- Starting eval on experience 4 (Task 0) from test stream --\n",
      "100%|██████████| 16/16 [00:00<00:00, 193.25it/s]\n",
      "> Eval on experience 4 (Task 0) from test stream ended.\n",
      "\tLoss_Exp/eval_phase/test_stream/Task000/Exp004 = 0.1174\n",
      "\tTop1_Acc_Exp/eval_phase/test_stream/Task000/Exp004 = 0.9607\n",
      "-- >> End of eval phase << --\n",
      "\tLoss_Stream/eval_phase/test_stream/Task000 = 11.3223\n",
      "\tTop1_Acc_Stream/eval_phase/test_stream/Task000 = 0.1905\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:24<00:00, 12.02s/it]\n"
     ]
    }
   ],
   "source": [
    "# Train ensembles - single guru\n",
    "\n",
    "one_active_exp = Experiment(\n",
    "    n_trials=num_trials,\n",
    "    ensembles=list(ensembles_dict.values()),\n",
    "    benchmark=data,\n",
    "    strategies_to_evaluate=initialize_strategies_to_evaluate,\n",
    ")\n",
    "_ = one_active_exp.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save and Print Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch metric values for full-ensemble and batch_test_acc are: [[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.9765625, 0.9609375, 0.96875, 0.96875, 0.9765625, 0.953125, 0.96875, 0.96875, 0.984375, 0.9765625, 0.984375, 0.96875, 1.0, 0.9765625, 0.96875, 0.9841269850730896], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.9609375, 0.9765625, 0.9921875, 0.9921875, 0.9453125, 0.953125, 0.984375, 0.9453125, 0.984375, 0.9765625, 0.96875, 0.984375, 0.96875, 0.9609375, 0.953125, 0.9841269850730896]]\n",
      "Batch metric values for full-ensemble and batch_train_acc are: [[0.9921875, 1.0, 1.0, 1.0, 1.0, 0.984375, 0.9921875, 0.984375, 0.9921875, 0.9921875, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9921875, 0.9921875, 1.0, 1.0, 1.0, 0.9921875, 0.9921875, 0.9921875, 1.0, 0.9921875, 1.0, 1.0, 1.0, 0.984375, 1.0, 0.9921875, 1.0, 0.9921875, 1.0, 0.9921875, 0.984375, 1.0, 1.0, 1.0, 1.0, 0.9921875, 1.0, 0.9921875, 1.0, 0.9921875, 1.0, 1.0, 1.0, 1.0, 0.9921875, 1.0, 1.0, 1.0, 0.9921875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9921875, 0.9921875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9921875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9921875, 0.9921875, 0.9921875, 0.9921875, 1.0, 0.9921875, 1.0, 1.0, 1.0, 1.0, 0.9921875, 1.0, 1.0, 0.9921875, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0625, 0.59375, 0.78125, 0.828125, 0.8046875, 0.796875, 0.9140625, 0.8984375, 0.890625, 0.9296875, 0.921875, 0.9375, 0.9375, 0.8984375, 0.9609375, 0.9140625, 0.953125, 0.9453125, 0.921875, 0.8984375, 0.9375, 0.9921875, 0.9453125, 0.921875, 0.96875, 0.96875, 0.953125, 0.9765625, 0.9453125, 0.9765625, 0.96875, 0.96875, 0.9765625, 0.96875, 0.9296875, 0.9921875, 0.953125, 0.953125, 0.984375, 0.9765625, 0.984375, 0.9765625, 0.984375, 0.953125, 0.9765625, 0.9921875, 0.9453125, 0.9453125, 0.953125, 0.953125, 0.9765625, 0.9765625, 0.953125, 0.9921875, 0.984375, 0.96875, 0.9296875, 0.9921875, 0.9375, 0.9765625, 0.9765625, 0.9609375, 0.96875, 0.9765625, 0.9453125, 0.9765625, 0.9765625, 0.9765625, 0.9765625, 0.96875, 0.9453125, 0.9765625, 0.9921875, 0.9609375, 0.984375, 0.9609375, 0.984375, 0.953125, 0.9609375, 0.984375, 0.9453125, 0.953125, 0.9765625, 0.96875, 0.953125, 0.9765625, 0.96875, 0.96875, 0.9921875, 0.9649122953414917, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.03125, 0.1953125, 0.5234375, 0.4765625, 0.4921875, 0.5234375, 0.4765625, 0.5234375, 0.5078125, 0.53125, 0.5546875, 0.6796875, 0.7109375, 0.828125, 0.8125, 0.7265625, 0.6875, 0.7890625, 0.8828125, 0.90625, 0.96875, 0.953125, 0.9375, 0.9375, 0.96875, 0.9296875, 0.9765625, 0.921875, 0.9296875, 0.953125, 0.9375, 0.9765625, 0.984375, 0.9921875, 0.9609375, 0.9609375, 0.9609375, 0.96875, 0.96875, 0.953125, 0.9609375, 0.984375, 0.9921875, 0.9765625, 0.96875, 0.9609375, 0.984375, 1.0, 0.984375, 0.984375, 0.984375, 0.984375, 0.984375, 0.96875, 0.984375, 0.9765625, 0.984375, 0.953125, 1.0, 1.0, 0.9921875, 0.9921875, 1.0, 0.9921875, 0.9765625, 0.9921875, 0.9921875, 1.0, 0.984375, 1.0, 0.9921875, 0.984375, 1.0, 0.9921875, 0.984375, 1.0, 0.9921875, 0.9921875, 0.9763779640197754, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0078125, 0.2421875, 0.75, 0.9296875, 0.9609375, 0.953125, 0.9609375, 0.953125, 0.9453125, 0.96875, 0.9765625, 0.953125, 0.9453125, 0.9765625, 0.9609375, 0.96875, 0.9765625, 0.9921875, 0.9765625, 1.0, 0.9921875, 1.0, 0.9921875, 0.984375, 1.0, 0.9921875, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9921875, 0.9921875, 1.0, 1.0, 1.0, 1.0, 0.9921875, 1.0, 0.9921875, 0.9921875, 1.0, 0.984375, 0.9921875, 1.0, 1.0, 0.984375, 0.984375, 0.9765625, 0.984375, 0.9921875, 0.9921875, 1.0, 1.0, 0.9921875, 1.0, 1.0, 1.0, 1.0, 0.9921875, 0.9921875, 1.0, 0.984375, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9921875, 0.9921875, 1.0, 0.9921875, 0.984375, 1.0, 1.0, 1.0, 1.0, 0.9921875, 1.0, 1.0, 1.0, 0.9921875, 1.0, 0.9921875, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0234375, 0.1953125, 0.3671875, 0.46875, 0.671875, 0.78125, 0.8046875, 0.859375, 0.859375, 0.8359375, 0.890625, 0.9296875, 0.9140625, 0.921875, 0.90625, 0.921875, 0.875, 0.9375, 0.9453125, 0.9609375, 0.953125, 0.953125, 0.9609375, 0.9765625, 0.9921875, 0.9453125, 0.9375, 0.9609375, 0.921875, 0.9296875, 0.9609375, 0.9453125, 0.9453125, 0.96875, 0.96875, 0.96875, 0.9609375, 0.984375, 0.96875, 0.96875, 0.9375, 0.96875, 0.96875, 0.9765625, 1.0, 0.953125, 0.96875, 0.953125, 0.953125, 0.9375, 0.984375, 0.9375, 0.953125, 0.9609375, 0.953125, 0.96875, 0.9453125, 0.9765625, 0.9765625, 0.953125, 0.96875, 0.9609375, 0.9765625, 0.9609375, 0.953125, 1.0, 0.96875, 0.96875, 0.9921875, 0.9765625, 0.9765625, 0.96875, 0.9765625, 0.984375, 0.96875, 0.9921875, 0.9609375, 0.984375, 0.9166666865348816], [0.9921875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9921875, 0.9921875, 0.9921875, 0.9921875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9921875, 1.0, 1.0, 1.0, 1.0, 0.9921875, 1.0, 0.9921875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9921875, 1.0, 0.9921875, 1.0, 1.0, 1.0, 0.9921875, 1.0, 0.9921875, 1.0, 1.0, 1.0, 0.984375, 0.9921875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9921875, 1.0, 0.984375, 1.0, 0.9921875, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9765625, 1.0, 0.9921875, 1.0, 1.0, 1.0, 0.984375, 1.0, 0.9921875, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9921875, 0.9921875, 1.0, 0.9921875, 1.0, 1.0, 1.0, 0.9921875, 1.0, 1.0, 1.0, 1.0, 0.9921875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.140625, 0.5625, 0.4453125, 0.484375, 0.7109375, 0.890625, 0.9140625, 0.9453125, 0.9140625, 0.9296875, 0.9375, 0.9140625, 0.8671875, 0.9296875, 0.9453125, 0.921875, 0.953125, 0.921875, 0.9296875, 0.921875, 0.921875, 0.9296875, 0.9453125, 0.9296875, 0.9609375, 0.96875, 0.9765625, 0.953125, 0.9375, 0.9453125, 0.953125, 0.9453125, 0.96875, 0.9609375, 0.9453125, 0.96875, 0.9609375, 0.96875, 0.9765625, 0.953125, 0.984375, 0.984375, 0.9609375, 0.9609375, 0.96875, 0.9609375, 0.9453125, 1.0, 0.96875, 0.9375, 0.9765625, 0.96875, 0.9453125, 0.9453125, 0.9765625, 0.9765625, 0.9765625, 0.9453125, 0.9375, 0.96875, 0.96875, 0.984375, 0.9765625, 0.984375, 0.984375, 0.953125, 0.96875, 0.9765625, 0.96875, 0.9609375, 0.96875, 0.9921875, 0.984375, 0.9765625, 0.953125, 0.9609375, 0.984375, 0.9765625, 0.953125, 0.9765625, 0.9921875, 0.9453125, 0.9765625, 0.9296875, 0.9765625, 0.984375, 0.9765625, 0.953125, 0.953125, 0.96875, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0703125, 0.3203125, 0.6796875, 0.765625, 0.7265625, 0.7734375, 0.703125, 0.6875, 0.71875, 0.6328125, 0.625, 0.6484375, 0.6875, 0.6953125, 0.8046875, 0.8125, 0.90625, 0.8828125, 0.921875, 0.96875, 0.9375, 0.953125, 0.9375, 0.9375, 0.96875, 0.9453125, 0.984375, 0.9375, 0.9453125, 0.9765625, 0.9609375, 0.96875, 0.984375, 0.984375, 0.9765625, 0.984375, 0.96875, 0.9765625, 0.9765625, 0.96875, 0.96875, 0.96875, 0.9921875, 1.0, 0.984375, 1.0, 0.96875, 0.96875, 0.96875, 0.9765625, 1.0, 0.953125, 0.96875, 0.9921875, 0.9921875, 0.9921875, 0.9921875, 0.9921875, 0.9921875, 0.984375, 1.0, 0.9921875, 1.0, 0.9921875, 0.9765625, 0.984375, 0.9921875, 0.9921875, 0.9765625, 1.0, 0.984375, 0.984375, 0.9765625, 0.9921875, 0.9921875, 0.9765625, 1.0, 0.9921875, 0.984375, 0.9763779640197754, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0546875, 0.2890625, 0.6328125, 0.8359375, 0.859375, 0.8828125, 0.9453125, 0.96875, 0.9921875, 1.0, 0.9765625, 0.984375, 0.96875, 0.984375, 0.9765625, 0.9921875, 0.9921875, 1.0, 0.984375, 0.9921875, 0.9921875, 0.9921875, 1.0, 0.9921875, 0.9921875, 1.0, 0.984375, 0.9921875, 0.9921875, 0.984375, 1.0, 1.0, 0.9921875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9921875, 0.9921875, 1.0, 1.0, 1.0, 1.0, 0.9921875, 0.9921875, 1.0, 1.0, 1.0, 1.0, 0.9765625, 1.0, 1.0, 1.0, 0.9921875, 1.0, 1.0, 0.9921875, 1.0, 1.0, 1.0, 1.0, 0.9921875, 1.0, 0.9921875, 0.9921875, 1.0, 1.0, 0.9921875, 0.9921875, 0.9921875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.95652174949646, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0625, 0.359375, 0.546875, 0.7109375, 0.8359375, 0.8359375, 0.7890625, 0.8359375, 0.828125, 0.71875, 0.5703125, 0.640625, 0.640625, 0.53125, 0.625, 0.6015625, 0.6328125, 0.734375, 0.6953125, 0.6953125, 0.703125, 0.6953125, 0.6953125, 0.765625, 0.8671875, 0.875, 0.921875, 0.8984375, 0.9296875, 0.9453125, 0.9140625, 0.8828125, 0.8671875, 0.8515625, 0.8984375, 0.9140625, 0.890625, 0.9453125, 0.9609375, 0.921875, 0.9453125, 0.9609375, 0.9296875, 0.96875, 0.953125, 0.96875, 0.9296875, 0.9375, 0.953125, 0.9921875, 0.9921875, 0.9921875, 0.96875, 0.9765625, 0.96875, 0.9609375, 0.9375, 0.953125, 0.9609375, 0.953125, 0.9609375, 0.9765625, 0.984375, 0.953125, 0.96875, 0.9296875, 0.9609375, 0.96875, 0.9296875, 0.9765625, 0.9375, 0.9921875, 0.953125, 0.96875, 0.9765625, 0.9921875, 0.9921875, 0.9609375, 1.0]]\n",
      "Batch metric values for max_diversity-accuracy_score and batch_test_acc are: [[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0234375, 0.0078125, 0.0234375, 0.0234375, 0.0234375, 0.015625, 0.0234375, 0.03125, 0.015625, 0.015625, 0.0390625, 0.0078125, 0.0078125, 0.0234375, 0.03125, 0.016393441706895828, 0.171875, 0.1796875, 0.234375, 0.2421875, 0.171875, 0.140625, 0.171875, 0.25, 0.21875, 0.1796875, 0.1484375, 0.171875, 0.2421875, 0.2265625, 0.25609755516052246, 0.28125, 0.453125, 0.4453125, 0.3515625, 0.4609375, 0.3828125, 0.3671875, 0.3125, 0.2890625, 0.390625, 0.3671875, 0.375, 0.359375, 0.3671875, 0.296875, 0.39393940567970276, 0.8671875, 0.859375, 0.875, 0.9140625, 0.8671875, 0.859375, 0.875, 0.84375, 0.90625, 0.890625, 0.921875, 0.8828125, 0.9375, 0.9140625, 0.875, 0.8571428656578064], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.046875, 0.015625, 0.03125, 0.0234375, 0.03125, 0.015625, 0.0078125, 0.03125, 0.0234375, 0.0078125, 0.0234375, 0.0078125, 0.03125, 0.0078125, 0.0390625, 0.008196720853447914, 0.0546875, 0.0234375, 0.0, 0.015625, 0.03125, 0.0078125, 0.015625, 0.0234375, 0.015625, 0.015625, 0.03125, 0.015625, 0.046875, 0.0078125, 0.03658536449074745, 0.984375, 0.9921875, 0.9921875, 1.0, 0.9765625, 0.984375, 0.9921875, 0.9921875, 0.984375, 0.9921875, 0.984375, 0.9921875, 1.0, 0.984375, 0.9921875, 0.9696969985961914, 0.0390625, 0.0234375, 0.046875, 0.0234375, 0.046875, 0.0078125, 0.015625, 0.03125, 0.0078125, 0.015625, 0.03125, 0.03125, 0.0234375, 0.015625, 0.0078125, 0.0]]\n",
      "Batch metric values for max_diversity-accuracy_score and batch_train_acc are: [[0.9921875, 1.0, 1.0, 1.0, 1.0, 0.984375, 1.0, 0.984375, 0.9921875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9921875, 0.9921875, 1.0, 1.0, 0.9921875, 0.9921875, 0.9921875, 0.9921875, 0.9921875, 0.9921875, 1.0, 1.0, 1.0, 0.984375, 1.0, 0.9921875, 1.0, 0.9921875, 0.9921875, 0.9921875, 0.984375, 1.0, 1.0, 1.0, 1.0, 0.9921875, 1.0, 0.9921875, 1.0, 0.9921875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9921875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9921875, 0.9921875, 0.9921875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9921875, 0.9921875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.984375, 0.9921875, 0.984375, 1.0, 0.9921875, 1.0, 1.0, 1.0, 0.9921875, 0.9921875, 1.0, 1.0, 0.9921875, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.3046875, 0.7578125, 0.890625, 0.90625, 0.9375, 0.90625, 0.9140625, 0.9375, 0.921875, 0.9296875, 0.9375, 0.8828125, 0.9453125, 0.9296875, 0.9609375, 0.96875, 0.9140625, 0.9140625, 0.9375, 0.984375, 0.953125, 0.9375, 0.9609375, 0.9609375, 0.953125, 0.9765625, 0.9375, 0.0, 0.9609375, 0.96875, 0.0, 0.96875, 0.0, 0.0, 0.9453125, 0.0, 0.984375, 0.96875, 0.984375, 0.0, 0.9921875, 0.953125, 0.96875, 0.9921875, 0.9453125, 0.9453125, 0.96875, 0.953125, 0.9765625, 0.96875, 0.9609375, 0.9765625, 0.9765625, 0.9609375, 0.9375, 0.9921875, 0.9296875, 0.96875, 0.984375, 0.9609375, 0.96875, 0.9765625, 0.9453125, 0.9765625, 0.9765625, 0.9765625, 0.96875, 0.9765625, 0.96875, 0.984375, 0.9921875, 0.9609375, 0.984375, 0.96875, 0.984375, 0.953125, 0.96875, 0.984375, 0.9609375, 0.96875, 0.9765625, 0.96875, 0.953125, 0.96875, 0.96875, 0.9765625, 1.0, 0.9824561476707458, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.2109375, 0.609375, 0.546875, 0.5234375, 0.5625, 0.578125, 0.484375, 0.4921875, 0.515625, 0.4765625, 0.515625, 0.5390625, 0.6328125, 0.859375, 0.9453125, 0.96875, 0.96875, 0.9765625, 0.9453125, 0.9609375, 0.953125, 0.9453125, 0.9375, 0.96875, 0.984375, 0.953125, 0.9453125, 0.9765625, 0.953125, 0.9765625, 0.9453125, 0.96875, 0.9609375, 0.953125, 0.96875, 0.9921875, 0.9921875, 0.9609375, 0.9765625, 0.96875, 0.96875, 0.984375, 0.9453125, 0.9765625, 0.9765625, 1.0, 0.984375, 0.9921875, 0.9609375, 0.9921875, 1.0, 0.984375, 0.984375, 0.6796875, 0.984375, 0.9921875, 0.9765625, 0.984375, 0.984375, 0.984375, 0.953125, 1.0, 1.0, 0.9921875, 0.9921875, 0.15625, 0.375, 0.421875, 0.53125, 0.7421875, 0.734375, 0.8359375, 0.9453125, 0.953125, 0.984375, 0.90625, 0.9140625, 0.9453125, 0.9296875, 0.953125, 0.96875, 0.9448819160461426, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.125, 0.453125, 0.8984375, 0.921875, 0.8828125, 0.8203125, 0.8359375, 0.8359375, 0.796875, 0.9375, 0.8828125, 0.8984375, 0.9140625, 0.953125, 0.953125, 0.9453125, 0.90625, 0.984375, 0.96875, 0.9921875, 0.984375, 1.0, 0.984375, 1.0, 1.0, 0.984375, 1.0, 0.9921875, 0.984375, 0.9921875, 1.0, 0.9921875, 0.9921875, 0.9921875, 1.0, 1.0, 1.0, 0.984375, 0.984375, 1.0, 0.9921875, 1.0, 0.9765625, 1.0, 1.0, 1.0, 0.984375, 0.984375, 0.9765625, 0.984375, 0.9921875, 0.9921875, 1.0, 0.9921875, 0.9921875, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9921875, 1.0, 0.984375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9921875, 1.0, 0.9921875, 0.984375, 1.0, 1.0, 1.0, 1.0, 0.9921875, 1.0, 1.0, 1.0, 0.9921875, 0.9921875, 0.9921875, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.1015625, 0.265625, 0.5859375, 0.6796875, 0.8203125, 0.84375, 0.8515625, 0.9140625, 0.90625, 0.921875, 0.9609375, 0.9765625, 0.9296875, 0.96875, 0.9296875, 0.921875, 0.9375, 0.96875, 0.921875, 0.9453125, 0.9609375, 0.9453125, 0.9453125, 0.9375, 0.96875, 0.9375, 0.953125, 0.953125, 0.9765625, 0.9765625, 0.9921875, 0.96875, 0.9296875, 0.9609375, 0.953125, 0.953125, 0.984375, 0.953125, 0.9453125, 0.953125, 0.9765625, 0.9765625, 0.9609375, 0.9921875, 0.9765625, 0.96875, 0.9609375, 0.9765625, 0.984375, 0.9609375, 0.9921875, 0.9765625, 0.9765625, 0.953125, 0.9609375, 0.953125, 0.96875, 0.96875, 0.9609375, 0.9609375, 0.953125, 0.9765625, 0.96875, 0.984375, 0.96875, 0.96875, 0.984375, 0.9609375, 0.9921875, 0.96875, 0.9609375, 1.0, 0.9765625, 0.9765625, 0.984375, 0.9765625, 0.984375, 0.9765625, 0.984375, 0.9921875, 0.9765625, 0.9921875, 0.9609375, 0.9765625, 0.9583333134651184], [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9921875, 0.9921875, 0.9921875, 0.9921875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9921875, 1.0, 1.0, 1.0, 1.0, 0.9921875, 1.0, 0.9921875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9921875, 1.0, 0.9921875, 1.0, 1.0, 1.0, 0.9921875, 1.0, 0.9921875, 1.0, 1.0, 1.0, 0.984375, 0.9921875, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9921875, 1.0, 0.9921875, 1.0, 0.984375, 1.0, 0.9921875, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9921875, 1.0, 1.0, 1.0, 1.0, 1.0, 0.984375, 1.0, 0.9921875, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9921875, 0.9921875, 1.0, 0.984375, 1.0, 1.0, 1.0, 0.9921875, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9921875, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0078125, 0.375, 0.0, 0.53125, 0.578125, 0.6953125, 0.7734375, 0.8671875, 0.890625, 0.9296875, 0.875, 0.90625, 0.890625, 0.921875, 0.921875, 0.8671875, 0.875, 0.8984375, 0.875, 0.953125, 0.96875, 0.96875, 0.9375, 0.921875, 0.9140625, 0.9296875, 0.921875, 0.0, 0.9609375, 0.9375, 0.953125, 0.953125, 0.9453125, 0.9609375, 0.9296875, 0.9921875, 0.9765625, 0.9609375, 0.9609375, 0.9609375, 0.9609375, 0.9375, 1.0, 0.9609375, 0.9375, 0.9609375, 0.96875, 0.9609375, 0.9375, 0.96875, 0.9765625, 0.96875, 0.953125, 0.921875, 0.953125, 0.9609375, 0.984375, 0.9765625, 0.96875, 0.984375, 0.96875, 0.9609375, 0.96875, 0.9765625, 0.9453125, 0.96875, 0.96875, 0.9921875, 0.984375, 0.953125, 0.953125, 0.9765625, 0.984375, 0.9609375, 0.9765625, 0.9921875, 0.953125, 0.96875, 0.9140625, 0.390625, 0.703125, 0.90625, 0.859375, 0.875, 0.9140625, 0.9649122953414917, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.125, 0.40625, 0.734375, 0.8515625, 0.9140625, 0.8671875, 0.8984375, 0.859375, 0.8203125, 0.8046875, 0.7578125, 0.8125, 0.765625, 0.8828125, 0.8984375, 0.8984375, 0.90625, 0.921875, 0.96875, 0.9296875, 0.9765625, 0.9296875, 0.90625, 0.9609375, 0.9375, 0.984375, 0.921875, 0.921875, 0.9765625, 0.96875, 0.984375, 0.9921875, 0.9921875, 0.984375, 0.984375, 0.9765625, 0.9765625, 0.9765625, 0.9609375, 0.96875, 0.9609375, 0.9921875, 1.0, 0.9921875, 0.9921875, 0.96875, 0.953125, 0.96875, 0.9765625, 0.9921875, 0.953125, 0.9765625, 1.0, 0.9921875, 1.0, 0.9921875, 0.9921875, 0.9921875, 0.9765625, 1.0, 0.9921875, 1.0, 0.984375, 0.9765625, 0.984375, 0.984375, 0.9921875, 0.9765625, 1.0, 0.984375, 0.9765625, 0.96875, 0.9921875, 0.984375, 0.9765625, 0.9921875, 0.9921875, 0.984375, 0.9763779640197754, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.5546875, 0.8046875, 0.8515625, 0.9609375, 0.90625, 0.953125, 0.9453125, 0.9609375, 0.96875, 0.9921875, 1.0, 0.9765625, 0.9921875, 0.9921875, 1.0, 0.9765625, 1.0, 1.0, 0.9921875, 0.9921875, 0.9921875, 1.0, 0.9921875, 0.9921875, 0.9921875, 1.0, 1.0, 0.9921875, 0.984375, 1.0, 1.0, 0.984375, 1.0, 0.9921875, 0.9765625, 1.0, 1.0, 0.9921875, 0.984375, 1.0, 1.0, 0.9921875, 1.0, 1.0, 1.0, 0.5703125, 0.9921875, 0.984375, 1.0, 1.0, 1.0, 1.0, 0.53125, 0.7734375, 0.90625, 0.9765625, 0.984375, 0.9921875, 0.984375, 1.0, 0.9921875, 0.9921875, 0.96875, 0.9921875, 1.0, 0.984375, 0.96875, 1.0, 0.9609375, 1.0, 1.0, 1.0, 0.984375, 0.9765625, 1.0, 0.9921875, 0.9921875, 0.9921875, 0.9921875, 1.0, 1.0, 1.0, 1.0, 0.9921875, 1.0, 1.0, 1.0, 1.0, 0.95652174949646, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0859375, 0.421875, 0.5703125, 0.484375, 0.546875, 0.5390625, 0.5703125, 0.59375, 0.7421875, 0.71875, 0.7421875, 0.921875, 0.9765625, 0.9375, 0.8984375, 0.9375, 0.9140625, 0.953125, 0.9140625, 0.96875, 0.90625, 0.9765625, 0.9375, 0.96875, 0.96875, 0.9453125, 0.953125, 0.9375, 0.9609375, 0.9609375, 0.9609375, 0.9375, 0.8828125, 0.9140625, 0.9140625, 0.9609375, 0.9296875, 0.9609375, 0.9921875, 0.953125, 0.96875, 0.9609375, 0.9375, 0.984375, 0.96875, 0.9921875, 0.9453125, 0.9453125, 0.96875, 1.0, 0.9921875, 0.9921875, 0.9765625, 0.984375, 0.9765625, 0.9609375, 0.9375, 0.9609375, 0.96875, 0.96875, 0.953125, 0.9765625, 0.9921875, 0.9609375, 0.9765625, 0.9375, 0.984375, 0.96875, 0.953125, 0.984375, 0.9453125, 0.984375, 0.9609375, 0.984375, 0.984375, 0.9921875, 0.9921875, 0.953125, 1.0]]\n",
      "Batch metric values for LwF and batch_test_acc are: [[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.9453125, 0.90625, 0.9453125, 0.921875, 0.9453125, 0.9609375, 0.984375, 0.9453125, 0.9609375, 0.9296875, 0.96875, 0.9765625, 0.984375, 0.96875, 1.0, 0.9841269841269841], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.9296875, 0.921875, 0.9609375, 0.953125, 0.953125, 0.953125, 0.9765625, 0.9453125, 0.96875, 0.9375, 0.953125, 0.984375, 0.984375, 0.9921875, 0.9921875, 0.9682539682539683]]\n",
      "Batch metric values for LwF and batch_train_acc are: [[0.046875, 0.9921875, 1.0, 1.0, 0.9921875, 0.9921875, 1.0, 1.0, 1.0, 1.0, 0.9921875, 0.9921875, 1.0, 0.9921875, 0.9921875, 1.0, 1.0, 0.984375, 1.0, 1.0, 0.9921875, 0.9921875, 0.9921875, 1.0, 1.0, 1.0, 0.9921875, 1.0, 1.0, 0.9921875, 0.984375, 0.9921875, 1.0, 0.9921875, 1.0, 1.0, 0.9921875, 0.9921875, 1.0, 0.9921875, 1.0, 1.0, 1.0, 1.0, 0.9921875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9921875, 1.0, 0.984375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9921875, 0.9921875, 1.0, 0.9921875, 1.0, 1.0, 1.0, 1.0, 0.9921875, 1.0, 1.0, 1.0, 0.9921875, 1.0, 1.0, 1.0, 0.9921875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9921875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.03125, 0.1953125, 0.3671875, 0.4140625, 0.5390625, 0.4921875, 0.4921875, 0.484375, 0.609375, 0.6484375, 0.7265625, 0.8046875, 0.890625, 0.875, 0.9140625, 0.8828125, 0.9375, 0.9375, 0.8984375, 0.953125, 0.9453125, 0.921875, 0.921875, 0.9765625, 0.90625, 0.9375, 0.9375, 0.9375, 0.96875, 0.9609375, 0.921875, 0.9453125, 0.921875, 0.96875, 0.96875, 0.953125, 0.9453125, 0.953125, 0.9296875, 0.8984375, 0.953125, 0.90625, 0.96875, 0.96875, 0.8984375, 0.9140625, 0.953125, 0.9609375, 0.9375, 0.96875, 0.96875, 0.984375, 0.953125, 0.9609375, 0.9921875, 0.96875, 0.9140625, 0.9609375, 0.953125, 0.9453125, 0.9609375, 0.9453125, 0.9609375, 0.9921875, 0.9375, 0.96875, 0.953125, 0.96875, 0.9609375, 0.9453125, 0.9609375, 0.953125, 0.96875, 0.984375, 0.9375, 0.953125, 0.9609375, 0.984375, 0.9765625, 0.9921875, 0.96875, 0.9375, 0.9609375, 0.9765625, 0.9765625, 0.9921875, 0.953125, 0.9649122807017544, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0078125, 0.015625, 0.0703125, 0.15625, 0.4375, 0.5703125, 0.71875, 0.7890625, 0.78125, 0.8515625, 0.8515625, 0.84375, 0.859375, 0.78125, 0.78125, 0.6953125, 0.734375, 0.7578125, 0.7890625, 0.765625, 0.7421875, 0.796875, 0.8515625, 0.8828125, 0.9140625, 0.953125, 0.921875, 0.9375, 0.9375, 0.9296875, 0.953125, 0.9609375, 0.9375, 0.953125, 0.9375, 0.96875, 0.9375, 0.953125, 0.9375, 0.984375, 0.921875, 0.9453125, 0.96875, 0.953125, 0.96875, 0.96875, 1.0, 0.9765625, 0.96875, 0.953125, 0.9609375, 0.9609375, 0.953125, 0.9609375, 0.9921875, 0.9765625, 0.9921875, 0.9375, 0.9921875, 0.9921875, 1.0, 0.9765625, 0.9765625, 0.96875, 0.9921875, 0.984375, 0.9609375, 0.984375, 0.9921875, 0.984375, 0.96875, 0.9763779527559056, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0234375, 0.046875, 0.1171875, 0.2578125, 0.3828125, 0.4609375, 0.4296875, 0.4765625, 0.6171875, 0.59375, 0.6796875, 0.65625, 0.7109375, 0.828125, 0.859375, 0.9140625, 0.8984375, 0.96875, 0.9765625, 0.9609375, 0.9765625, 1.0, 1.0, 0.9921875, 1.0, 1.0, 1.0, 0.984375, 1.0, 0.9765625, 0.984375, 0.9921875, 0.9921875, 0.9765625, 0.984375, 0.984375, 1.0, 0.984375, 0.9921875, 0.9921875, 1.0, 0.984375, 1.0, 0.9921875, 0.9765625, 0.9921875, 0.984375, 1.0, 1.0, 1.0, 0.9921875, 0.9921875, 1.0, 0.9921875, 0.9921875, 0.9921875, 1.0, 1.0, 1.0, 0.9921875, 0.9921875, 0.9921875, 1.0, 1.0, 0.9921875, 1.0, 1.0, 1.0, 0.9921875, 0.9921875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.015625, 0.0546875, 0.1015625, 0.1953125, 0.265625, 0.21875, 0.421875, 0.53125, 0.546875, 0.75, 0.7578125, 0.8671875, 0.9296875, 0.9453125, 0.953125, 0.96875, 0.9609375, 0.9140625, 0.953125, 0.8984375, 0.921875, 0.9453125, 0.9921875, 0.953125, 0.9375, 0.953125, 0.9453125, 0.9453125, 0.9375, 0.96875, 0.9765625, 0.9296875, 0.9453125, 0.9375, 0.953125, 0.9375, 0.9765625, 0.96875, 0.9765625, 0.9140625, 0.9765625, 0.921875, 0.9765625, 0.953125, 0.984375, 0.96875, 0.9609375, 0.9765625, 0.953125, 0.9453125, 0.9609375, 0.9453125, 0.9375, 0.9765625, 0.9375, 0.9765625, 0.9375, 0.984375, 0.9609375, 0.9583333333333334], [0.3359375, 0.984375, 0.9921875, 0.9921875, 1.0, 1.0, 1.0, 0.9921875, 1.0, 0.9921875, 0.9921875, 1.0, 0.9921875, 1.0, 0.9921875, 0.9921875, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9921875, 1.0, 1.0, 1.0, 0.9921875, 1.0, 0.9921875, 1.0, 0.9765625, 1.0, 1.0, 1.0, 0.9921875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9921875, 0.9921875, 0.9921875, 0.9921875, 1.0, 1.0, 1.0, 0.9921875, 1.0, 1.0, 0.9921875, 0.9921875, 1.0, 1.0, 0.9921875, 1.0, 0.9921875, 1.0, 0.9921875, 1.0, 1.0, 0.984375, 1.0, 1.0, 1.0, 0.9921875, 1.0, 1.0, 0.9921875, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9921875, 1.0, 1.0, 0.9921875, 1.0, 1.0, 0.9921875, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9921875, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9921875, 1.0, 1.0, 0.9917355371900827, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.015625, 0.140625, 0.4921875, 0.828125, 0.890625, 0.9140625, 0.890625, 0.9296875, 0.8671875, 0.8515625, 0.9140625, 0.859375, 0.875, 0.953125, 0.8984375, 0.9296875, 0.9375, 0.9296875, 0.921875, 0.9453125, 0.9453125, 0.9296875, 0.9140625, 0.9375, 0.9375, 0.9375, 0.9375, 0.953125, 0.9609375, 0.921875, 0.953125, 0.9609375, 0.9609375, 0.9453125, 0.953125, 0.921875, 0.9375, 0.9609375, 0.9296875, 0.9765625, 0.9453125, 0.921875, 0.953125, 0.9609375, 0.9765625, 0.96875, 0.9609375, 0.953125, 0.9609375, 0.9765625, 0.9765625, 0.9921875, 0.9609375, 0.921875, 0.9765625, 0.9765625, 0.9609375, 0.953125, 1.0, 0.9609375, 0.96875, 0.9765625, 0.953125, 0.96875, 0.953125, 0.984375, 0.984375, 0.96875, 0.9375, 0.9765625, 0.984375, 0.9765625, 0.9609375, 0.953125, 0.953125, 0.96875, 0.9609375, 0.9765625, 0.9453125, 0.9765625, 0.9765625, 0.96875, 0.96875, 0.9921875, 0.9453125, 0.9824561403508771, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.046875, 0.1171875, 0.1796875, 0.3359375, 0.453125, 0.7890625, 0.796875, 0.8046875, 0.890625, 0.890625, 0.9453125, 0.859375, 0.9140625, 0.9140625, 0.890625, 0.9296875, 0.9296875, 0.9453125, 0.9296875, 0.9296875, 0.9140625, 0.9453125, 0.9375, 0.921875, 0.9296875, 0.96875, 0.953125, 0.9609375, 0.96875, 0.9765625, 0.953125, 0.921875, 0.9921875, 0.9453125, 0.9296875, 0.96875, 0.9765625, 0.96875, 0.9609375, 0.9609375, 0.9609375, 0.984375, 0.9453125, 0.96875, 1.0, 0.9609375, 0.9921875, 0.9609375, 0.96875, 0.9765625, 0.984375, 0.9765625, 0.953125, 1.0, 0.9609375, 0.984375, 0.9921875, 0.984375, 0.953125, 0.9921875, 0.9921875, 0.984375, 0.9921875, 0.9765625, 1.0, 0.9921875, 1.0, 0.9763779527559056, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.03125, 0.109375, 0.265625, 0.390625, 0.5078125, 0.5546875, 0.7265625, 0.828125, 0.8125, 0.8671875, 0.90625, 0.9375, 0.90625, 0.90625, 0.9453125, 0.9296875, 0.8984375, 0.96875, 0.9765625, 0.984375, 0.9765625, 0.984375, 0.984375, 0.9921875, 0.9765625, 1.0, 0.984375, 0.9921875, 0.9921875, 0.984375, 0.9765625, 0.9921875, 1.0, 0.9921875, 1.0, 1.0, 0.9921875, 0.9921875, 1.0, 0.984375, 0.984375, 0.9921875, 0.9921875, 1.0, 1.0, 1.0, 0.9921875, 1.0, 0.984375, 0.9921875, 0.9921875, 1.0, 0.9921875, 0.9921875, 1.0, 0.9921875, 0.984375, 0.9921875, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9921875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.984375, 1.0, 0.9921875, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0078125, 0.109375, 0.2265625, 0.34375, 0.421875, 0.5234375, 0.4609375, 0.5078125, 0.453125, 0.5234375, 0.546875, 0.484375, 0.5078125, 0.515625, 0.515625, 0.5625, 0.4921875, 0.5078125, 0.453125, 0.53125, 0.4921875, 0.484375, 0.546875, 0.5390625, 0.6796875, 0.78125, 0.796875, 0.8671875, 0.9375, 0.8359375, 0.875, 0.8671875, 0.90625, 0.8671875, 0.9453125, 0.953125, 0.9609375, 0.9375, 0.9140625, 0.9609375, 0.875, 0.9375, 0.9609375, 0.9921875, 0.9453125, 0.9375, 0.890625, 0.9453125, 0.9765625, 0.9765625, 0.9375, 0.96875, 0.9140625, 0.9453125, 0.90625, 0.9375, 0.953125, 0.9921875, 0.953125, 0.953125, 0.953125, 0.9375, 0.9609375, 0.9296875, 0.984375, 0.9765625, 0.9765625, 0.9609375, 0.921875, 0.96875, 0.9765625, 0.96875, 0.9296875, 0.953125, 0.875]]\n",
      "                                     ensemble_name         0         1  \\\n",
      "batch_test_acc-mean                  full-ensemble  0.000000  0.000000   \n",
      "batch_test_acc-std                   full-ensemble  0.000000  0.000000   \n",
      "batch_train_acc-mean                 full-ensemble  0.992188  1.000000   \n",
      "batch_train_acc-std                  full-ensemble  0.000000  0.000000   \n",
      "batch_test_acc-mean   max_diversity-accuracy_score  0.000000  0.000000   \n",
      "batch_test_acc-std    max_diversity-accuracy_score  0.000000  0.000000   \n",
      "batch_train_acc-mean  max_diversity-accuracy_score  0.996094  1.000000   \n",
      "batch_train_acc-std   max_diversity-accuracy_score  0.003906  0.000000   \n",
      "batch_test_acc-mean                            LwF  0.000000  0.000000   \n",
      "batch_test_acc-std                             LwF  0.000000  0.000000   \n",
      "batch_train_acc-mean                           LwF  0.191406  0.988281   \n",
      "batch_train_acc-std                            LwF  0.144531  0.003906   \n",
      "\n",
      "                             2         3         4         5         6  \\\n",
      "batch_test_acc-mean   0.000000  0.000000  0.000000  0.000000  0.000000   \n",
      "batch_test_acc-std    0.000000  0.000000  0.000000  0.000000  0.000000   \n",
      "batch_train_acc-mean  1.000000  1.000000  1.000000  0.992188  0.996094   \n",
      "batch_train_acc-std   0.000000  0.000000  0.000000  0.007812  0.003906   \n",
      "batch_test_acc-mean   0.000000  0.000000  0.000000  0.000000  0.000000   \n",
      "batch_test_acc-std    0.000000  0.000000  0.000000  0.000000  0.000000   \n",
      "batch_train_acc-mean  1.000000  1.000000  1.000000  0.992188  1.000000   \n",
      "batch_train_acc-std   0.000000  0.000000  0.000000  0.007812  0.000000   \n",
      "batch_test_acc-mean   0.000000  0.000000  0.000000  0.000000  0.000000   \n",
      "batch_test_acc-std    0.000000  0.000000  0.000000  0.000000  0.000000   \n",
      "batch_train_acc-mean  0.996094  0.996094  0.996094  0.996094  1.000000   \n",
      "batch_train_acc-std   0.003906  0.003906  0.003906  0.003906  0.000000   \n",
      "\n",
      "                             7         8  ...       461       462       463  \\\n",
      "batch_test_acc-mean   0.000000  0.000000  ...       NaN       NaN       NaN   \n",
      "batch_test_acc-std    0.000000  0.000000  ...       NaN       NaN       NaN   \n",
      "batch_train_acc-mean  0.992188  0.996094  ...  0.976562  0.957031  0.980469   \n",
      "batch_train_acc-std   0.007812  0.003906  ...  0.000000  0.019531  0.011719   \n",
      "batch_test_acc-mean   0.000000  0.000000  ...       NaN       NaN       NaN   \n",
      "batch_test_acc-std    0.000000  0.000000  ...       NaN       NaN       NaN   \n",
      "batch_train_acc-mean  0.992188  0.996094  ...  0.980469  0.964844  0.980469   \n",
      "batch_train_acc-std   0.007812  0.003906  ...  0.003906  0.019531  0.003906   \n",
      "batch_test_acc-mean   0.000000  0.000000  ...       NaN       NaN       NaN   \n",
      "batch_test_acc-std    0.000000  0.000000  ...       NaN       NaN       NaN   \n",
      "batch_train_acc-mean  0.996094  1.000000  ...  0.968750  0.960938  0.949219   \n",
      "batch_train_acc-std   0.003906  0.000000  ...  0.007812  0.015625  0.011719   \n",
      "\n",
      "                           464       465       466       467       468  \\\n",
      "batch_test_acc-mean        NaN       NaN       NaN       NaN       NaN   \n",
      "batch_test_acc-std         NaN       NaN       NaN       NaN       NaN   \n",
      "batch_train_acc-mean  0.964844  0.976562  0.972656  0.992188  0.976562   \n",
      "batch_train_acc-std   0.011719  0.007812  0.003906  0.000000  0.015625   \n",
      "batch_test_acc-mean        NaN       NaN       NaN       NaN       NaN   \n",
      "batch_test_acc-std         NaN       NaN       NaN       NaN       NaN   \n",
      "batch_train_acc-mean  0.972656  0.988281  0.980469  0.992188  0.976562   \n",
      "batch_train_acc-std   0.011719  0.003906  0.003906  0.000000  0.015625   \n",
      "batch_test_acc-mean        NaN       NaN       NaN       NaN       NaN   \n",
      "batch_test_acc-std         NaN       NaN       NaN       NaN       NaN   \n",
      "batch_train_acc-mean  0.949219  0.953125  0.976562  0.953125  0.957031   \n",
      "batch_train_acc-std   0.027344  0.015625  0.000000  0.015625  0.027344   \n",
      "\n",
      "                           469       470  \n",
      "batch_test_acc-mean        NaN       NaN  \n",
      "batch_test_acc-std         NaN       NaN  \n",
      "batch_train_acc-mean  0.972656  0.958333  \n",
      "batch_train_acc-std   0.011719  0.041667  \n",
      "batch_test_acc-mean        NaN       NaN  \n",
      "batch_test_acc-std         NaN       NaN  \n",
      "batch_train_acc-mean  0.964844  0.979167  \n",
      "batch_train_acc-std   0.011719  0.020833  \n",
      "batch_test_acc-mean        NaN       NaN  \n",
      "batch_test_acc-std         NaN       NaN  \n",
      "batch_train_acc-mean  0.957031  0.916667  \n",
      "batch_train_acc-std   0.003906  0.041667  \n",
      "\n",
      "[12 rows x 472 columns]\n"
     ]
    }
   ],
   "source": [
    "batch_metrics = one_active_exp.get_aggregate_batch_metrics()\n",
    "dfs = []\n",
    "for ens, metric_dict in batch_metrics.items():\n",
    "    df = pd.DataFrame.from_dict(metric_dict, orient=\"index\")\n",
    "    df[\"ensemble_name\"] = ens\n",
    "    dfs.append(df)\n",
    "df = pd.concat(dfs)\n",
    "col_order = [len(df.columns) - 1] + list(range(len(df.columns) - 1))\n",
    "df = df[df.columns[col_order]]\n",
    "print(df)\n",
    "file_prefix = f\"class_incremental_single_guru-trials={num_trials}-batch_size={batch_size}_window_size={window_size}\"\n",
    "path = \"results\"\n",
    "\n",
    "df.to_csv(f\"{path}/{file_prefix}.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results for mechanisms with max_active_gurus = 1:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean train acc for full-ensemble: 0.862+-0.017\n",
      "Mean train acc for max_diversity-accuracy_score: 0.863+-0.043\n",
      "Mean train acc for LwF: 0.767+-0.026\n",
      "--------------\n",
      "Mean test acc for full-ensemble: 0.194+-0.002\n",
      "Mean test acc for max_diversity-accuracy_score: 0.252+-0.166\n",
      "Mean test acc for LwF: 0.767+-0.026\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ben/VSCode/banditLiquidDem/.conda/lib/python3.10/site-packages/avalanche/training/plugins/evaluation.py:68: UserWarning: No loggers specified, metrics will not be logged\n",
      "  warnings.warn(\"No loggers specified, metrics will not be logged\")\n"
     ]
    }
   ],
   "source": [
    "# Print results - single guru\n",
    "\n",
    "print(f\"Results for mechanisms with max_active_gurus = {max_active_gurus}:\")\n",
    "\n",
    "# Collect and print train accuracies - aggregate and by batch\n",
    "train_results_dict = dict()\n",
    "for ens_name, ensemble in ensembles_dict.items():\n",
    "    train_acc, train_acc_std = calculate_avg_std_train_accs(\n",
    "        one_active_exp, ens_name, num_trials\n",
    "    )\n",
    "    train_results_dict[ens_name] = (train_acc, train_acc_std)\n",
    "\n",
    "for strat_name, (strat, eval_plugin) in initialize_strategies_to_evaluate().items():\n",
    "    train_acc, train_acc_std = calculate_avg_std_train_accs(\n",
    "        one_active_exp, strat_name, num_trials\n",
    "    )\n",
    "    train_results_dict[strat_name] = (train_acc, train_acc_std)\n",
    "\n",
    "for ens_name, (train_acc, train_acc_std) in train_results_dict.items():\n",
    "    print(\n",
    "        f\"Mean train acc for {ens_name}: {round(np.mean(train_acc), 3)}+-{round(np.mean(train_acc_std), 3)}\"\n",
    "    )\n",
    "# for ens_name, (train_acc, train_acc_std) in train_results_dict.items():\n",
    "#     print(f\"All train accs for {ens_name}: {train_acc}\")\n",
    "\n",
    "print(\"--------------\")\n",
    "\n",
    "# Collect and print test accuracies\n",
    "results_dict = dict()\n",
    "for ens_name, ensemble in ensembles_dict.items():\n",
    "    test_acc, test_acc_std = calculate_avg_std_test_accs(\n",
    "        one_active_exp, ens_name, num_trials\n",
    "    )\n",
    "    results_dict[ens_name] = (test_acc, test_acc_std)\n",
    "\n",
    "for strat_name, (strat, eval_plugin) in initialize_strategies_to_evaluate().items():\n",
    "    test_acc, test_acc_std = calculate_avg_std_train_accs(\n",
    "        one_active_exp, strat_name, num_trials\n",
    "    )\n",
    "    results_dict[strat_name] = (test_acc, test_acc_std)\n",
    "\n",
    "for ens_name, (test_acc, test_acc_std) in results_dict.items():\n",
    "    print(\n",
    "        f\"Mean test acc for {ens_name}: {round(np.mean(test_acc), 3)}+-{round(np.mean(test_acc_std), 3)}\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Many Active Voters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Delegation Mechanisms - many gurus\n",
    "\n",
    "NOOP_del_mech = DelegationMechanism(batch_size=batch_size, window_size=window_size)\n",
    "\n",
    "probability_functions = [\n",
    "    \"random_better\",\n",
    "    \"probabilistic_better\",\n",
    "    \"probabilistic_weighted\",\n",
    "]\n",
    "score_functions = [\n",
    "    \"accuracy_score\",\n",
    "    \"balanced_accuracy_score\",\n",
    "    \"f1_score\",\n",
    "    \"precision_score\",\n",
    "    \"recall_score\",\n",
    "    \"top_k_accuracy_score\",\n",
    "    \"roc_auc_score\",\n",
    "    \"log_loss_score\",\n",
    "]\n",
    "max_active_gurus = 3\n",
    "\n",
    "del_mechs = {\"full-ensemble\": NOOP_del_mech}\n",
    "for prob_func, score_func in product(probability_functions, score_functions):\n",
    "    dm = ProbaSlopeDelegationMechanism(\n",
    "        batch_size=batch_size,\n",
    "        window_size=window_size,\n",
    "        max_active=max_active_gurus,\n",
    "        probability_function=prob_func,\n",
    "        score_method=score_func,\n",
    "    )\n",
    "    del_mechs[f\"{prob_func}-{score_func}\"] = dm\n",
    "\n",
    "\n",
    "many_active_ensembles_dict = {\n",
    "    dm_name: Ensemble(\n",
    "        training_epochs=1,\n",
    "        n_voters=n_voters,\n",
    "        delegation_mechanism=dm,\n",
    "        name=dm_name,\n",
    "        input_dim=28 * 28,\n",
    "        output_dim=10,\n",
    "    )\n",
    "    for dm_name, dm in del_mechs.items()\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Experiment.__init__() got an unexpected keyword argument 'data'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Run experiment - Many gurus\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m many_active_exp \u001b[38;5;241m=\u001b[39m \u001b[43mExperiment\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[43mn_trials\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_trials\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[43mensembles\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mmany_active_ensembles_dict\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalues\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[43mseed\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m4090\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m      9\u001b[0m _ \u001b[38;5;241m=\u001b[39m many_active_exp\u001b[38;5;241m.\u001b[39mrun()\n",
      "\u001b[0;31mTypeError\u001b[0m: Experiment.__init__() got an unexpected keyword argument 'data'"
     ]
    }
   ],
   "source": [
    "# Run experiment - Many gurus\n",
    "\n",
    "many_active_exp = Experiment(\n",
    "    n_trials=num_trials,\n",
    "    ensembles=list(many_active_ensembles_dict.values()),\n",
    "    data=data,\n",
    "    seed=4090,\n",
    ")\n",
    "_ = many_active_exp.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Results for mechanisms with max_active_gurus = {max_active_gurus}:\")\n",
    "\n",
    "# Collect and print train accuracies - aggregate and by batch\n",
    "train_results_dict = dict()\n",
    "for ens_name, ensemble in many_active_ensembles_dict.items():\n",
    "    train_acc, train_acc_std = calculate_avg_std_train_accs(\n",
    "        many_active_exp, ens_name, num_trials\n",
    "    )\n",
    "    train_results_dict[ens_name] = (train_acc, train_acc_std)\n",
    "\n",
    "for ens_name, (train_acc, train_acc_std) in train_results_dict.items():\n",
    "    print(\n",
    "        f\"Mean train acc for {ens_name}: {round(np.mean(train_acc), 3)}+-{round(np.mean(train_acc_std), 3)}\"\n",
    "    )\n",
    "# for ens_name, (train_acc, train_acc_std) in train_results_dict.items():\n",
    "#     print(f\"All train accs for {ens_name}: {train_acc}\")\n",
    "\n",
    "print(\"--------------\")\n",
    "\n",
    "# Collect and print test accuracies\n",
    "results_dict = dict()\n",
    "for ens_name, ensemble in many_active_ensembles_dict.items():\n",
    "    test_acc, test_acc_std = calculate_avg_std_test_accs(\n",
    "        many_active_exp, ens_name, num_trials\n",
    "    )\n",
    "    results_dict[ens_name] = (test_acc, test_acc_std)\n",
    "\n",
    "for ens_name, (test_acc, test_acc_std) in results_dict.items():\n",
    "    print(\n",
    "        f\"Mean test acc for {ens_name}: {round(np.mean(test_acc), 3)}+-{round(np.mean(test_acc_std), 3)}\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basic Comparison with Avalanche"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from exp_framework.learning import Net\n",
    "from torch.optim import SGD, Adam\n",
    "from torch.nn import CrossEntropyLoss\n",
    "from avalanche.models import SimpleMLP\n",
    "from avalanche.training.supervised import (\n",
    "    Naive,\n",
    "    CWRStar,\n",
    "    Replay,\n",
    "    GDumb,\n",
    "    Cumulative,\n",
    "    LwF,\n",
    "    GEM,\n",
    "    AGEM,\n",
    "    EWC,\n",
    ")  # and many more!\n",
    "from avalanche.benchmarks.classic import RotatedMNIST, SplitMNIST\n",
    "from avalanche.training.plugins import ReplayPlugin\n",
    "import pprint\n",
    "\n",
    "model = Net(input_dim=28 * 28, output_dim=10)\n",
    "optimize = Adam(model.parameters(), lr=0.001)\n",
    "replay = ReplayPlugin(mem_size=100)\n",
    "\n",
    "cl_strategy = Naive(\n",
    "    model,\n",
    "    optimizer=optimize,\n",
    "    criterion=CrossEntropyLoss(),\n",
    "    train_mb_size=128,\n",
    "    train_epochs=1,\n",
    "    eval_mb_size=128,\n",
    "    # plugins=[replay]\n",
    ")\n",
    "# optimizer = SGD(model.parameters(), lr=0.001, momentum=0.9)\n",
    "# model = SimpleMLP(num_classes=10)\n",
    "# criterion = CrossEntropyLoss()\n",
    "# cl_strategy = Naive(\n",
    "#     model, SGD(model.parameters(), lr=0.001, momentum=0.9), criterion,\n",
    "#     train_mb_size=100, train_epochs=4, eval_mb_size=100\n",
    "# )\n",
    "\n",
    "\n",
    "# scenario\n",
    "# benchmark = RotatedMNIST(n_experiences=5, seed=1)\n",
    "benchmark = SplitMNIST(n_experiences=5, fixed_class_order=list(range(10)), seed=1)\n",
    "\n",
    "# TRAINING LOOP\n",
    "print(\"Starting experiment...\")\n",
    "results = []\n",
    "for experience in benchmark.train_stream:\n",
    "    print(\"Start of experience: \", experience.current_experience)\n",
    "    print(\"Current Classes: \", experience.classes_in_this_experience)\n",
    "\n",
    "    cl_strategy.train(experience)\n",
    "    print(\"Training completed\")\n",
    "\n",
    "    print(\"Computing accuracy on the whole test set\")\n",
    "    results.append(cl_strategy.eval(benchmark.test_stream))\n",
    "\n",
    "for r in results:\n",
    "    pprint.pprint(r)\n",
    "# print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from avalanche.benchmarks.classic import RotatedMNIST\n",
    "\n",
    "# scenario\n",
    "benchmark = RotatedMNIST(n_experiences=5, seed=1)\n",
    "\n",
    "# TRAINING LOOP\n",
    "print(\"Starting experiment...\")\n",
    "results = []\n",
    "for experience in benchmark.train_stream:\n",
    "    print(\"Start of experience: \", experience.current_experience)\n",
    "    print(\"Current Classes: \", experience.classes_in_this_experience)\n",
    "\n",
    "    cl_strategy.train(experience)\n",
    "    print(\"Training completed\")\n",
    "\n",
    "    print(\"Computing accuracy on the whole test set\")\n",
    "    results.append(cl_strategy.eval(benchmark.test_stream))\n",
    "\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Explore Results\n",
    "\n",
    "(leftover copied code from other file, not adapted for the above code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set_style(\"whitegrid\")\n",
    "sns.set_context(\"paper\", font_scale=2)\n",
    "\n",
    "plt.rcParams[\"font.family\"] = \"serif\"\n",
    "plt.rcParams[\"font.serif\"] = \"Georgia\"\n",
    "\n",
    "# set colors for each bar. Use pastel\n",
    "colors = sns.color_palette(\"pastel\")\n",
    "# assign colors for each bar\n",
    "\n",
    "fig = plt.figure(figsize=(10, 5))\n",
    "ax = fig.add_subplot(111)\n",
    "ax.set_title(\"Test Accuracies\")\n",
    "ax.set_ylabel(\"Test Accuracy\")\n",
    "ax.set_xlabel(\"Delegation Mechanism\")\n",
    "# ax.set_xticks([0, 1, 2])\n",
    "# ax.set_xticklabels(\n",
    "#     [\"No Delegation\", \"Proba Slope\", \"Restricted Max Guru\"], rotation=45, ha=\"right\"\n",
    "# )\n",
    "ax.set_xticks([0, 1])\n",
    "ax.set_xticklabels([\"No Delegation\", \"Proba Slope\"], rotation=45, ha=\"right\")\n",
    "# Data for bar plot\n",
    "means = [\n",
    "    np.mean(full_avg_test_accs),\n",
    "    np.mean(proba_slope_avg_test_accs),\n",
    "    # np.mean(restricted_max_guru_avg_test_accs),\n",
    "]\n",
    "stds = [\n",
    "    np.std(full_avg_test_accs),\n",
    "    np.std(proba_slope_avg_test_accs),\n",
    "    # np.std(restricted_max_guru_avg_test_accs),\n",
    "]\n",
    "\n",
    "# Create each bar individually to set different colors\n",
    "for i in range(len(ensembles)):\n",
    "    ax.bar(i, means[i], color=colors[i], yerr=stds[i], capsize=10)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "proba_slope_avg_train_accs, proba_slope_std_train_accs = calculate_avg_std_train_accs(\n",
    "    exp, \"proba_slope_delegations\", num_trials\n",
    ")\n",
    "full_avg_train_accs, full_std_train_accs = calculate_avg_std_train_accs(\n",
    "    exp, \"full_ensemble\", num_trials\n",
    ")\n",
    "\n",
    "# (\n",
    "#     restricted_max_guru_avg_train_accs,\n",
    "#     restricted_max_guru_std_train_accs,\n",
    "# ) = calculate_avg_std_train_accs(exp, \"restricted_max_guru_delegations\", num_trials)\n",
    "\n",
    "print(\n",
    "    \"Mean train accs for proba_slope delegation ensemble: \",\n",
    "    np.mean(proba_slope_avg_train_accs),\n",
    ")\n",
    "print(\"Mean train accs for full ensemble: \", np.mean(full_avg_train_accs))\n",
    "\n",
    "# print(\n",
    "#     \"Mean train accs for restricted_max_guru delegation ensemble: \",\n",
    "#     np.mean(restricted_max_guru_avg_train_accs),\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_splits = exp.train_splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set(style=\"whitegrid\", palette=\"pastel\", context=\"paper\")\n",
    "\n",
    "# Set the font to Georgia\n",
    "mpl.rcParams[\"font.family\"] = \"Georgia\"\n",
    "mpl.rcParams[\"font.size\"] = 12\n",
    "mpl.rcParams[\"axes.labelsize\"] = 14\n",
    "mpl.rcParams[\"axes.titlesize\"] = 16\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 5))\n",
    "\n",
    "colors = sns.color_palette(\"pastel\")\n",
    "proba_slope_color = colors[1]\n",
    "full_color = colors[0]\n",
    "restricted_max_guru_color = colors[2]\n",
    "\n",
    "ax.plot(\n",
    "    proba_slope_avg_train_accs,\n",
    "    label=\"ProbaSlope Delegation Ensemble\",\n",
    "    color=proba_slope_color,\n",
    "    linewidth=2,\n",
    ")\n",
    "ax.fill_between(\n",
    "    range(len(proba_slope_avg_train_accs)),\n",
    "    np.array(proba_slope_avg_train_accs) - np.array(proba_slope_std_train_accs),\n",
    "    np.array(proba_slope_avg_train_accs) + np.array(proba_slope_std_train_accs),\n",
    "    color=proba_slope_color,\n",
    "    alpha=0.3,\n",
    ")\n",
    "\n",
    "ax.plot(full_avg_train_accs, label=\"Full Ensemble\", color=full_color, linewidth=2)\n",
    "ax.fill_between(\n",
    "    range(len(full_avg_train_accs)),\n",
    "    np.array(full_avg_train_accs) - np.array(full_std_train_accs),\n",
    "    np.array(full_avg_train_accs) + np.array(full_std_train_accs),\n",
    "    color=full_color,\n",
    "    alpha=0.3,\n",
    ")\n",
    "\n",
    "# ax.plot(\n",
    "#     restricted_max_guru_avg_train_accs,\n",
    "#     label=\"Restricted Max Guru Delegation Ensemble\",\n",
    "#     color=restricted_max_guru_color,\n",
    "#     linewidth=2,\n",
    "# )\n",
    "# ax.fill_between(\n",
    "#     range(len(restricted_max_guru_avg_train_accs)),\n",
    "#     np.array(restricted_max_guru_avg_train_accs)\n",
    "#     - np.array(restricted_max_guru_std_train_accs),\n",
    "#     np.array(restricted_max_guru_avg_train_accs)\n",
    "#     + np.array(restricted_max_guru_std_train_accs),\n",
    "#     color=colors[2],\n",
    "#     alpha=0.3,\n",
    "# )\n",
    "\n",
    "\n",
    "# plot vertical lines at test splits\n",
    "for split in train_splits[:-1]:\n",
    "    ax.axvline(x=split, color=\"k\", linestyle=\"--\", linewidth=1)\n",
    "\n",
    "# Setting labels, title, and legend\n",
    "ax.set_xlabel(\"Batch Number\")\n",
    "ax.set_ylabel(\"Train Accuracy\")\n",
    "ax.set_title(\n",
    "    \"ProbaSlope Delegation Ensemble vs Full Ensemble vs Restricted Max Guru Delegation Ensemble\"\n",
    ")\n",
    "\n",
    "ax.legend(loc=\"upper left\")\n",
    "# set y lim to lower\n",
    "ax.set_ylim(top=1.3)\n",
    "# set y ticks to 0-1\n",
    "ax.set_yticks(np.arange(0, 1.1, 0.1))\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ps_voters = exp.ensembles[1].voters\n",
    "print(ensembles[1].name)\n",
    "batch_accs = []\n",
    "for v in ps_voters:\n",
    "    batch_accs.append(v.batch_accuracies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len_train = len(data.train_data_loader.dataset) / batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_active_streaks(voter_id, trial_num):\n",
    "    \"\"\"\n",
    "    Find active streaks for a specified voter.\n",
    "\n",
    "    :param voter_id: ID of the voter for which to find active streaks.\n",
    "    :param batch_metric_values: Dictionary containing the batch metric values.\n",
    "    :param metric_key: Key to access the relevant metric in batch_metric_values.\n",
    "    :return: List of active streaks for the specified voter.\n",
    "    \"\"\"\n",
    "    active_batches = []\n",
    "    active_streak = [None, None]\n",
    "    voter_active = False\n",
    "\n",
    "    for i, av in enumerate(\n",
    "        exp.batch_metric_values[\"proba_slope_delegations\"][trial_num][\n",
    "            \"active_voters-train\"\n",
    "        ]\n",
    "    ):\n",
    "        # print(av)\n",
    "        if voter_id in av:\n",
    "            if not voter_active:\n",
    "                # Start a new streak\n",
    "                active_streak[0] = i\n",
    "                voter_active = True\n",
    "                # print(\"streak started\")\n",
    "            active_streak[1] = i\n",
    "        else:\n",
    "            if voter_active:\n",
    "                # End the current streak\n",
    "                active_batches.append(active_streak.copy())\n",
    "                active_streak = [None, None]\n",
    "                voter_active = False\n",
    "                # print(\"streak done\")\n",
    "\n",
    "    # Handle case where the streak continues till the end of the list\n",
    "    if voter_active:\n",
    "        active_batches.append(active_streak.copy())\n",
    "\n",
    "    return active_batches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Look at activity on last trial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for voter_id in range(n_voters):\n",
    "    active_streaks = find_active_streaks(voter_id, num_trials - 1)\n",
    "    # print(f\"Active Streaks for Voter {voter_id}: {active_streaks}\")\n",
    "\n",
    "    plt.figure(figsize=(10, 5))  # Create a new figure for each voter\n",
    "    plt.plot(batch_accs[voter_id])\n",
    "    plt.axvline(x=len_train, color=\"k\", linestyle=\"--\", linewidth=1)\n",
    "\n",
    "    # Shade the active batches for this voter\n",
    "    for streak in active_streaks:\n",
    "        if streak[0] is not None and streak[1] is not None:\n",
    "            plt.axvspan(streak[0], streak[1], alpha=0.3, color=\"red\")\n",
    "\n",
    "    # Plot a green vertical line at all train splits\n",
    "    for split in train_splits[:-1]:\n",
    "        plt.axvline(x=split, color=\"g\", linestyle=\"--\", linewidth=2)\n",
    "\n",
    "    plt.title(f\"Voter {voter_id} Activity\")\n",
    "    plt.xlabel(\"Batches\")\n",
    "    plt.ylabel(\"Accuracy\")\n",
    "    plt.show()  # Display the plot for each voter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "LDE",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
