{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ben/VSCode/banditLiquidDem/.conda/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from exp_framework.Ensemble import Ensemble, PretrainedEnsemble, StudentExpertEnsemble\n",
    "from exp_framework.delegation import (\n",
    "    DelegationMechanism,\n",
    "    UCBDelegationMechanism,\n",
    "    ProbaSlopeDelegationMechanism,\n",
    "    RestrictedMaxGurusDelegationMechanism,\n",
    "    StudentExpertDelegationMechanism,\n",
    ")\n",
    "from exp_framework.experiment import (\n",
    "    Experiment,\n",
    "    calculate_avg_std_test_accs,\n",
    "    calculate_avg_std_train_accs,\n",
    ")\n",
    "from matplotlib import pyplot as plt\n",
    "from exp_framework.data_utils import Data\n",
    "from avalanche.benchmarks.classic import RotatedMNIST, SplitMNIST\n",
    "import numpy as np\n",
    "import matplotlib as mpl\n",
    "import seaborn as sns\n",
    "from itertools import product\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### learning the mapping $\\mathcal{X} \\rightarrow \\mathcal{G}$ (i.e. $\\mathcal{X} \\rightarrow \\mathcal{Y}\\times\\mathcal{C}$)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "window_size = 50\n",
    "num_trials = 3\n",
    "n_voters = 10\n",
    "\n",
    "\n",
    "# # Set up the Class Incremental framework\n",
    "# data = Data(\n",
    "#     data_set_name=\"mnist\",\n",
    "#     # train_digit_groups=[range(5), range(5, 10)],\n",
    "#     # train_digit_groups=[[0, 1, 2], [3, 4, 5,], [6, 7, 8, 9]],\n",
    "#     train_digit_groups=[[0, 1], [2, 3], [4, 5], [6, 7], [8, 9]],\n",
    "#     # test_digit_groups=[[0, 1], [2, 3], [4, 5], [6, 7], [8, 9]],\n",
    "#     # test_digit_groups=[range(5), range(5, 10)],\n",
    "#     test_digit_groups=[range(10)],\n",
    "#     batch_size=batch_size,\n",
    "# )\n",
    "\n",
    "data = SplitMNIST(n_experiences=5, fixed_class_order=list(range(10)))\n",
    "# if data == \"MNIST\":\n",
    "#     benchmark = SplitMNIST(n_experiences=5, fixed_class_order=list(range(10)), seed=self.seed)\n",
    "# elif data == \"RotatedMNIST\":\n",
    "#     benchmark = RotatedMNIST(n_experiences=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Single Active Voter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Delegation Mechanisms and Ensembles\n",
    "\n",
    "For simplicity, only explore full ensemble and variants of ProbaSlopeDelegationMechanism since they can be created programmatically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Delegation Mechanisms - single guru\n",
    "\n",
    "NOOP_del_mech = DelegationMechanism(batch_size=batch_size, window_size=window_size)\n",
    "\n",
    "probability_functions = [\"random_better\", \"probabilistic_better\", \"probabilistic_weighted\"]\n",
    "score_functions = [\n",
    "                   \"accuracy_score\",\n",
    "                   \"balanced_accuracy_score\",\n",
    "                   \"f1_score\",\n",
    "                   \"precision_score\",\n",
    "                   \"recall_score\",\n",
    "                   \"top_k_accuracy_score\",\n",
    "                   \"roc_auc_score\",\n",
    "                   \"log_loss_score\",\n",
    "                   \"max_diversity\",\n",
    "                   ]\n",
    "probability_functions = [\"max_diversity\"]\n",
    "score_functions = [\n",
    "                   \"accuracy_score\"\n",
    "                   ]\n",
    "max_active_gurus = 1\n",
    "\n",
    "del_mechs = {\n",
    "    \"full-ensemble\": NOOP_del_mech\n",
    "}\n",
    "for prob_func, score_func in product(probability_functions, score_functions):\n",
    "    dm = ProbaSlopeDelegationMechanism(\n",
    "        batch_size=batch_size,\n",
    "        window_size=window_size,\n",
    "        max_active=max_active_gurus,\n",
    "        probability_function=prob_func,\n",
    "        score_method=score_func\n",
    "        )\n",
    "    del_mechs[f\"{prob_func}-{score_func}\"] = dm\n",
    "\n",
    "\n",
    "ensembles_dict = {\n",
    "    dm_name: Ensemble(\n",
    "        training_epochs=1,\n",
    "        n_voters=n_voters,\n",
    "        delegation_mechanism=dm,\n",
    "        name=dm_name,\n",
    "        input_dim=28 * 28,\n",
    "        output_dim=10,\n",
    "    )\n",
    "    for dm_name, dm in del_mechs.items()\n",
    "}\n",
    "\n",
    "# restricted_max_gurus_mech = RestrictedMaxGurusDelegationMechanism(\n",
    "#     batch_size=batch_size,\n",
    "#     num_voters=n_voters,\n",
    "#     max_active_voters=max_active_gurus,\n",
    "#     window_size=window_size,\n",
    "#     t_between_delegation=3,\n",
    "# )\n",
    "# UCB_del_mech = UCBDelegationMechanism(\n",
    "#     batch_size=batch_size,\n",
    "#     window_size=window_size,\n",
    "#     ucb_window_size=None\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Ensemble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 33%|███▎      | 1/3 [00:21<00:43, 21.56s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Train ensembles - single guru\u001b[39;00m\n\u001b[1;32m      3\u001b[0m one_active_exp \u001b[38;5;241m=\u001b[39m Experiment(n_trials\u001b[38;5;241m=\u001b[39mnum_trials, ensembles\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mlist\u001b[39m(ensembles_dict\u001b[38;5;241m.\u001b[39mvalues()), benchmark\u001b[38;5;241m=\u001b[39mdata)\n\u001b[0;32m----> 4\u001b[0m _ \u001b[38;5;241m=\u001b[39m \u001b[43mone_active_exp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/VSCode/banditLiquidDem/exp_framework/experiment.py:73\u001b[0m, in \u001b[0;36mExperiment.run\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     69\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m tqdm(\u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_trials)):\n\u001b[1;32m     70\u001b[0m     \u001b[38;5;66;03m# Set seed for reproducibility\u001b[39;00m\n\u001b[1;32m     71\u001b[0m     seed_everything(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mseed \u001b[38;5;241m+\u001b[39m t)\n\u001b[0;32m---> 73\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msingle_trial\u001b[49m\u001b[43m(\u001b[49m\u001b[43mt\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     75\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatch_metric_values\n",
      "File \u001b[0;32m~/VSCode/banditLiquidDem/exp_framework/experiment.py:133\u001b[0m, in \u001b[0;36mExperiment.single_trial\u001b[0;34m(self, trial_num)\u001b[0m\n\u001b[1;32m    125\u001b[0m \u001b[38;5;66;03m# if batch_idx in self.train_splits:\u001b[39;00m\n\u001b[1;32m    126\u001b[0m \u001b[38;5;66;03m#     if self.verbose:\u001b[39;00m\n\u001b[1;32m    127\u001b[0m \u001b[38;5;66;03m#         print(\u001b[39;00m\n\u001b[1;32m    128\u001b[0m \u001b[38;5;66;03m#             f\"Switching from digit group {self.train_digit_groups[current_digit_group]} to {self.train_digit_groups[current_digit_group+1]}\"\u001b[39;00m\n\u001b[1;32m    129\u001b[0m \u001b[38;5;66;03m#         )\u001b[39;00m\n\u001b[1;32m    130\u001b[0m \u001b[38;5;66;03m#     current_digit_group += 1\u001b[39;00m\n\u001b[1;32m    131\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m ensemble \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mensembles:  \u001b[38;5;66;03m# TODO: use train_models from ensemble.py?\u001b[39;00m\n\u001b[1;32m    132\u001b[0m     \u001b[38;5;66;03m# Train on a batch of data\u001b[39;00m\n\u001b[0;32m--> 133\u001b[0m     \u001b[43mensemble\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlearn_batch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    135\u001b[0m     \u001b[38;5;66;03m# Record performance - do this before any delegation so there's at least some data\u001b[39;00m\n\u001b[1;32m    136\u001b[0m     train_acc \u001b[38;5;241m=\u001b[39m ensemble\u001b[38;5;241m.\u001b[39mscore(images, labels, record_pointwise_accs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "File \u001b[0;32m~/VSCode/banditLiquidDem/exp_framework/Ensemble.py:212\u001b[0m, in \u001b[0;36mEnsemble.learn_batch\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m    210\u001b[0m loss \u001b[38;5;241m=\u001b[39m voter\u001b[38;5;241m.\u001b[39mcriterion(logits, labels)\n\u001b[1;32m    211\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[0;32m--> 212\u001b[0m \u001b[43mvoter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/VSCode/banditLiquidDem/.conda/lib/python3.10/site-packages/torch/optim/optimizer.py:373\u001b[0m, in \u001b[0;36mOptimizer.profile_hook_step.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    368\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    369\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m    370\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must return None or a tuple of (new_args, new_kwargs), but got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresult\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    371\u001b[0m             )\n\u001b[0;32m--> 373\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    374\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_optimizer_step_code()\n\u001b[1;32m    376\u001b[0m \u001b[38;5;66;03m# call optimizer step post hooks\u001b[39;00m\n",
      "File \u001b[0;32m~/VSCode/banditLiquidDem/.conda/lib/python3.10/site-packages/torch/optim/optimizer.py:76\u001b[0m, in \u001b[0;36m_use_grad_for_differentiable.<locals>._use_grad\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     74\u001b[0m     torch\u001b[38;5;241m.\u001b[39mset_grad_enabled(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdefaults[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdifferentiable\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m     75\u001b[0m     torch\u001b[38;5;241m.\u001b[39m_dynamo\u001b[38;5;241m.\u001b[39mgraph_break()\n\u001b[0;32m---> 76\u001b[0m     ret \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     77\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     78\u001b[0m     torch\u001b[38;5;241m.\u001b[39m_dynamo\u001b[38;5;241m.\u001b[39mgraph_break()\n",
      "File \u001b[0;32m~/VSCode/banditLiquidDem/.conda/lib/python3.10/site-packages/torch/optim/adam.py:163\u001b[0m, in \u001b[0;36mAdam.step\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    152\u001b[0m     beta1, beta2 \u001b[38;5;241m=\u001b[39m group[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbetas\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m    154\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_init_group(\n\u001b[1;32m    155\u001b[0m         group,\n\u001b[1;32m    156\u001b[0m         params_with_grad,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    160\u001b[0m         max_exp_avg_sqs,\n\u001b[1;32m    161\u001b[0m         state_steps)\n\u001b[0;32m--> 163\u001b[0m     \u001b[43madam\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    164\u001b[0m \u001b[43m        \u001b[49m\u001b[43mparams_with_grad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    165\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    166\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexp_avgs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    167\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    168\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_exp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    169\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    170\u001b[0m \u001b[43m        \u001b[49m\u001b[43mamsgrad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mamsgrad\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    171\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbeta1\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta1\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    172\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbeta2\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta2\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    173\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mlr\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    174\u001b[0m \u001b[43m        \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mweight_decay\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    175\u001b[0m \u001b[43m        \u001b[49m\u001b[43meps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43meps\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    176\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmaximize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmaximize\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    177\u001b[0m \u001b[43m        \u001b[49m\u001b[43mforeach\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mforeach\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    178\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcapturable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcapturable\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    179\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdifferentiable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mdifferentiable\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    180\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfused\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mfused\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    181\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgrad_scale\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mgrad_scale\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    182\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfound_inf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfound_inf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    183\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    185\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m loss\n",
      "File \u001b[0;32m~/VSCode/banditLiquidDem/.conda/lib/python3.10/site-packages/torch/optim/adam.py:311\u001b[0m, in \u001b[0;36madam\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, foreach, capturable, differentiable, fused, grad_scale, found_inf, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize)\u001b[0m\n\u001b[1;32m    308\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    309\u001b[0m     func \u001b[38;5;241m=\u001b[39m _single_tensor_adam\n\u001b[0;32m--> 311\u001b[0m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    312\u001b[0m \u001b[43m     \u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    313\u001b[0m \u001b[43m     \u001b[49m\u001b[43mexp_avgs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    314\u001b[0m \u001b[43m     \u001b[49m\u001b[43mexp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    315\u001b[0m \u001b[43m     \u001b[49m\u001b[43mmax_exp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    316\u001b[0m \u001b[43m     \u001b[49m\u001b[43mstate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    317\u001b[0m \u001b[43m     \u001b[49m\u001b[43mamsgrad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mamsgrad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    318\u001b[0m \u001b[43m     \u001b[49m\u001b[43mbeta1\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta1\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    319\u001b[0m \u001b[43m     \u001b[49m\u001b[43mbeta2\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta2\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    320\u001b[0m \u001b[43m     \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlr\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    321\u001b[0m \u001b[43m     \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mweight_decay\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    322\u001b[0m \u001b[43m     \u001b[49m\u001b[43meps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    323\u001b[0m \u001b[43m     \u001b[49m\u001b[43mmaximize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmaximize\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    324\u001b[0m \u001b[43m     \u001b[49m\u001b[43mcapturable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcapturable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    325\u001b[0m \u001b[43m     \u001b[49m\u001b[43mdifferentiable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdifferentiable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    326\u001b[0m \u001b[43m     \u001b[49m\u001b[43mgrad_scale\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgrad_scale\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    327\u001b[0m \u001b[43m     \u001b[49m\u001b[43mfound_inf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfound_inf\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/VSCode/banditLiquidDem/.conda/lib/python3.10/site-packages/torch/optim/adam.py:385\u001b[0m, in \u001b[0;36m_single_tensor_adam\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, grad_scale, found_inf, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize, capturable, differentiable)\u001b[0m\n\u001b[1;32m    383\u001b[0m \u001b[38;5;66;03m# Decay the first and second moment running average coefficient\u001b[39;00m\n\u001b[1;32m    384\u001b[0m exp_avg\u001b[38;5;241m.\u001b[39mlerp_(grad, \u001b[38;5;241m1\u001b[39m \u001b[38;5;241m-\u001b[39m beta1)\n\u001b[0;32m--> 385\u001b[0m \u001b[43mexp_avg_sq\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmul_\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbeta2\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39maddcmul_(grad, grad\u001b[38;5;241m.\u001b[39mconj(), value\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m-\u001b[39m beta2)\n\u001b[1;32m    387\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m capturable \u001b[38;5;129;01mor\u001b[39;00m differentiable:\n\u001b[1;32m    388\u001b[0m     step \u001b[38;5;241m=\u001b[39m step_t\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Train ensembles - single guru\n",
    "\n",
    "one_active_exp = Experiment(n_trials=num_trials, ensembles=list(ensembles_dict.values()), benchmark=data)\n",
    "_ = one_active_exp.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save and Print Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_metrics = one_active_exp.get_aggregate_batch_metrics()\n",
    "dfs = []\n",
    "for ens, metric_dict in batch_metrics.items():\n",
    "    df = pd.DataFrame.from_dict(metric_dict, orient='index')\n",
    "    df[\"ensemble_name\"] = ens\n",
    "    dfs.append(df)\n",
    "df = pd.concat(dfs)\n",
    "col_order = [len(df.columns)-1] + list(range(len(df.columns)-1))\n",
    "df = df[df.columns[col_order]]\n",
    "print(df)\n",
    "file_prefix = f\"class_incremental_single_guru-trials={num_trials}-batch_size={batch_size}_window_size={window_size}\"\n",
    "path = \"results\"\n",
    "\n",
    "df.to_csv(f\"{path}/{file_prefix}.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print results - single guru\n",
    "\n",
    "print(f\"Results for mechanisms with max_active_gurus = {max_active_gurus}:\")\n",
    "\n",
    "# Collect and print train accuracies - aggregate and by batch\n",
    "train_results_dict = dict()\n",
    "for ens_name, ensemble in ensembles_dict.items():\n",
    "    train_acc, train_acc_std = calculate_avg_std_train_accs(\n",
    "        one_active_exp, ens_name, num_trials\n",
    "    )\n",
    "    train_results_dict[ens_name] = (train_acc, train_acc_std)\n",
    "\n",
    "for ens_name, (train_acc, train_acc_std) in train_results_dict.items():\n",
    "    print(f\"Mean train acc for {ens_name}: {round(np.mean(train_acc), 3)}+-{round(np.mean(train_acc_std), 3)}\")\n",
    "# for ens_name, (train_acc, train_acc_std) in train_results_dict.items():\n",
    "#     print(f\"All train accs for {ens_name}: {train_acc}\")\n",
    "    \n",
    "print(\"--------------\")\n",
    "\n",
    "# Collect and print test accuracies\n",
    "results_dict = dict()\n",
    "for ens_name, ensemble in ensembles_dict.items():\n",
    "    test_acc, test_acc_std = calculate_avg_std_test_accs(\n",
    "        one_active_exp, ens_name, num_trials\n",
    "    )\n",
    "    results_dict[ens_name] = (test_acc, test_acc_std)\n",
    "\n",
    "for ens_name, (test_acc, test_acc_std) in results_dict.items():\n",
    "    print(f\"Mean test acc for {ens_name}: {round(np.mean(test_acc), 3)}+-{round(np.mean(test_acc_std), 3)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Many Active Voters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Delegation Mechanisms - single guru\n",
    "\n",
    "NOOP_del_mech = DelegationMechanism(batch_size=batch_size, window_size=window_size)\n",
    "\n",
    "probability_functions = [\"random_better\", \"probabilistic_better\", \"probabilistic_weighted\"]\n",
    "score_functions = [\n",
    "                   \"accuracy_score\",\n",
    "                   \"balanced_accuracy_score\",\n",
    "                   \"f1_score\",\n",
    "                   \"precision_score\",\n",
    "                   \"recall_score\",\n",
    "                   \"top_k_accuracy_score\",\n",
    "                   \"roc_auc_score\",\n",
    "                   \"log_loss_score\",\n",
    "                   ]\n",
    "max_active_gurus = 3\n",
    "\n",
    "del_mechs = {\n",
    "    \"full-ensemble\": NOOP_del_mech\n",
    "}\n",
    "for prob_func, score_func in product(probability_functions, score_functions):\n",
    "    dm = ProbaSlopeDelegationMechanism(\n",
    "        batch_size=batch_size,\n",
    "        window_size=window_size,\n",
    "        max_active=max_active_gurus,\n",
    "        probability_function=prob_func,\n",
    "        score_method=score_func\n",
    "        )\n",
    "    del_mechs[f\"{prob_func}-{score_func}\"] = dm\n",
    "\n",
    "\n",
    "many_active_ensembles_dict = {\n",
    "    dm_name: Ensemble(\n",
    "        training_epochs=1,\n",
    "        n_voters=n_voters,\n",
    "        delegation_mechanism=dm,\n",
    "        name=dm_name,\n",
    "        input_dim=28 * 28,\n",
    "        output_dim=10,\n",
    "    )\n",
    "    for dm_name, dm in del_mechs.items()\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run experiment - Many gurus\n",
    "\n",
    "many_active_exp = Experiment(n_trials=num_trials, ensembles=list(many_active_ensembles_dict.values()), data=data, seed=4090)\n",
    "_ = many_active_exp.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Results for mechanisms with max_active_gurus = {max_active_gurus}:\")\n",
    "\n",
    "# Collect and print train accuracies - aggregate and by batch\n",
    "train_results_dict = dict()\n",
    "for ens_name, ensemble in many_active_ensembles_dict.items():\n",
    "    train_acc, train_acc_std = calculate_avg_std_train_accs(\n",
    "        many_active_exp, ens_name, num_trials\n",
    "    )\n",
    "    train_results_dict[ens_name] = (train_acc, train_acc_std)\n",
    "\n",
    "for ens_name, (train_acc, train_acc_std) in train_results_dict.items():\n",
    "    print(f\"Mean train acc for {ens_name}: {round(np.mean(train_acc), 3)}+-{round(np.mean(train_acc_std), 3)}\")\n",
    "# for ens_name, (train_acc, train_acc_std) in train_results_dict.items():\n",
    "#     print(f\"All train accs for {ens_name}: {train_acc}\")\n",
    "    \n",
    "print(\"--------------\")\n",
    "\n",
    "# Collect and print test accuracies\n",
    "results_dict = dict()\n",
    "for ens_name, ensemble in many_active_ensembles_dict.items():\n",
    "    test_acc, test_acc_std = calculate_avg_std_test_accs(\n",
    "        many_active_exp, ens_name, num_trials\n",
    "    )\n",
    "    results_dict[ens_name] = (test_acc, test_acc_std)\n",
    "\n",
    "for ens_name, (test_acc, test_acc_std) in results_dict.items():\n",
    "    print(f\"Mean test acc for {ens_name}: {round(np.mean(test_acc), 3)}+-{round(np.mean(test_acc_std), 3)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basic Comparison with Avalanche"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from exp_framework.learning import Net\n",
    "from torch.optim import SGD, Adam\n",
    "from torch.nn import CrossEntropyLoss\n",
    "from avalanche.models import SimpleMLP\n",
    "from avalanche.training.supervised import Naive, CWRStar, Replay, GDumb, Cumulative, LwF, GEM, AGEM, EWC  # and many more!\n",
    "from avalanche.benchmarks.classic import RotatedMNIST, SplitMNIST\n",
    "from avalanche.training.plugins import ReplayPlugin\n",
    "import pprint\n",
    "\n",
    "model = Net(input_dim=28 * 28, output_dim=10)\n",
    "optimize = Adam(model.parameters(), lr=0.001)\n",
    "replay = ReplayPlugin(mem_size=100)\n",
    "\n",
    "cl_strategy = Naive(\n",
    "    model, optimizer=optimize, criterion=CrossEntropyLoss(),\n",
    "    train_mb_size=128, train_epochs=1, eval_mb_size=128,\n",
    "    # plugins=[replay]\n",
    ")\n",
    "# optimizer = SGD(model.parameters(), lr=0.001, momentum=0.9)\n",
    "# model = SimpleMLP(num_classes=10)\n",
    "# criterion = CrossEntropyLoss()\n",
    "# cl_strategy = Naive(\n",
    "#     model, SGD(model.parameters(), lr=0.001, momentum=0.9), criterion,\n",
    "#     train_mb_size=100, train_epochs=4, eval_mb_size=100\n",
    "# )\n",
    "\n",
    "\n",
    "# scenario\n",
    "# benchmark = RotatedMNIST(n_experiences=5, seed=1)\n",
    "benchmark = SplitMNIST(n_experiences=5, fixed_class_order=list(range(10)), seed=1)\n",
    "\n",
    "# TRAINING LOOP\n",
    "print('Starting experiment...')\n",
    "results = []\n",
    "for experience in benchmark.train_stream:\n",
    "    print(\"Start of experience: \", experience.current_experience)\n",
    "    print(\"Current Classes: \", experience.classes_in_this_experience)\n",
    "\n",
    "    cl_strategy.train(experience)\n",
    "    print('Training completed')\n",
    "\n",
    "    print('Computing accuracy on the whole test set')\n",
    "    results.append(cl_strategy.eval(benchmark.test_stream))\n",
    "\n",
    "for r in results:\n",
    "    pprint.pprint(r)\n",
    "# print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from avalanche.benchmarks.classic import RotatedMNIST\n",
    "\n",
    "# scenario\n",
    "benchmark = RotatedMNIST(n_experiences=5, seed=1)\n",
    "\n",
    "# TRAINING LOOP\n",
    "print('Starting experiment...')\n",
    "results = []\n",
    "for experience in benchmark.train_stream:\n",
    "    print(\"Start of experience: \", experience.current_experience)\n",
    "    print(\"Current Classes: \", experience.classes_in_this_experience)\n",
    "\n",
    "    cl_strategy.train(experience)\n",
    "    print('Training completed')\n",
    "\n",
    "    print('Computing accuracy on the whole test set')\n",
    "    results.append(cl_strategy.eval(benchmark.test_stream))\n",
    "\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Explore Results\n",
    "\n",
    "(leftover copied code from other file, not adapted for the above code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set_style(\"whitegrid\")\n",
    "sns.set_context(\"paper\", font_scale=2)\n",
    "\n",
    "plt.rcParams[\"font.family\"] = \"serif\"\n",
    "plt.rcParams[\"font.serif\"] = \"Georgia\"\n",
    "\n",
    "# set colors for each bar. Use pastel\n",
    "colors = sns.color_palette(\"pastel\")\n",
    "# assign colors for each bar\n",
    "\n",
    "fig = plt.figure(figsize=(10, 5))\n",
    "ax = fig.add_subplot(111)\n",
    "ax.set_title(\"Test Accuracies\")\n",
    "ax.set_ylabel(\"Test Accuracy\")\n",
    "ax.set_xlabel(\"Delegation Mechanism\")\n",
    "# ax.set_xticks([0, 1, 2])\n",
    "# ax.set_xticklabels(\n",
    "#     [\"No Delegation\", \"Proba Slope\", \"Restricted Max Guru\"], rotation=45, ha=\"right\"\n",
    "# )\n",
    "ax.set_xticks([0, 1])\n",
    "ax.set_xticklabels([\"No Delegation\", \"Proba Slope\"], rotation=45, ha=\"right\")\n",
    "# Data for bar plot\n",
    "means = [\n",
    "    np.mean(full_avg_test_accs),\n",
    "    np.mean(proba_slope_avg_test_accs),\n",
    "    # np.mean(restricted_max_guru_avg_test_accs),\n",
    "]\n",
    "stds = [\n",
    "    np.std(full_avg_test_accs),\n",
    "    np.std(proba_slope_avg_test_accs),\n",
    "    # np.std(restricted_max_guru_avg_test_accs),\n",
    "]\n",
    "\n",
    "# Create each bar individually to set different colors\n",
    "for i in range(len(ensembles)):\n",
    "    ax.bar(i, means[i], color=colors[i], yerr=stds[i], capsize=10)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "proba_slope_avg_train_accs, proba_slope_std_train_accs = calculate_avg_std_train_accs(\n",
    "    exp, \"proba_slope_delegations\", num_trials\n",
    ")\n",
    "full_avg_train_accs, full_std_train_accs = calculate_avg_std_train_accs(\n",
    "    exp, \"full_ensemble\", num_trials\n",
    ")\n",
    "\n",
    "# (\n",
    "#     restricted_max_guru_avg_train_accs,\n",
    "#     restricted_max_guru_std_train_accs,\n",
    "# ) = calculate_avg_std_train_accs(exp, \"restricted_max_guru_delegations\", num_trials)\n",
    "\n",
    "print(\n",
    "    \"Mean train accs for proba_slope delegation ensemble: \",\n",
    "    np.mean(proba_slope_avg_train_accs),\n",
    ")\n",
    "print(\"Mean train accs for full ensemble: \", np.mean(full_avg_train_accs))\n",
    "\n",
    "# print(\n",
    "#     \"Mean train accs for restricted_max_guru delegation ensemble: \",\n",
    "#     np.mean(restricted_max_guru_avg_train_accs),\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_splits = exp.train_splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set(style=\"whitegrid\", palette=\"pastel\", context=\"paper\")\n",
    "\n",
    "# Set the font to Georgia\n",
    "mpl.rcParams[\"font.family\"] = \"Georgia\"\n",
    "mpl.rcParams[\"font.size\"] = 12\n",
    "mpl.rcParams[\"axes.labelsize\"] = 14\n",
    "mpl.rcParams[\"axes.titlesize\"] = 16\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 5))\n",
    "\n",
    "colors = sns.color_palette(\"pastel\")\n",
    "proba_slope_color = colors[1]\n",
    "full_color = colors[0]\n",
    "restricted_max_guru_color = colors[2]\n",
    "\n",
    "ax.plot(\n",
    "    proba_slope_avg_train_accs,\n",
    "    label=\"ProbaSlope Delegation Ensemble\",\n",
    "    color=proba_slope_color,\n",
    "    linewidth=2,\n",
    ")\n",
    "ax.fill_between(\n",
    "    range(len(proba_slope_avg_train_accs)),\n",
    "    np.array(proba_slope_avg_train_accs) - np.array(proba_slope_std_train_accs),\n",
    "    np.array(proba_slope_avg_train_accs) + np.array(proba_slope_std_train_accs),\n",
    "    color=proba_slope_color,\n",
    "    alpha=0.3,\n",
    ")\n",
    "\n",
    "ax.plot(full_avg_train_accs, label=\"Full Ensemble\", color=full_color, linewidth=2)\n",
    "ax.fill_between(\n",
    "    range(len(full_avg_train_accs)),\n",
    "    np.array(full_avg_train_accs) - np.array(full_std_train_accs),\n",
    "    np.array(full_avg_train_accs) + np.array(full_std_train_accs),\n",
    "    color=full_color,\n",
    "    alpha=0.3,\n",
    ")\n",
    "\n",
    "# ax.plot(\n",
    "#     restricted_max_guru_avg_train_accs,\n",
    "#     label=\"Restricted Max Guru Delegation Ensemble\",\n",
    "#     color=restricted_max_guru_color,\n",
    "#     linewidth=2,\n",
    "# )\n",
    "# ax.fill_between(\n",
    "#     range(len(restricted_max_guru_avg_train_accs)),\n",
    "#     np.array(restricted_max_guru_avg_train_accs)\n",
    "#     - np.array(restricted_max_guru_std_train_accs),\n",
    "#     np.array(restricted_max_guru_avg_train_accs)\n",
    "#     + np.array(restricted_max_guru_std_train_accs),\n",
    "#     color=colors[2],\n",
    "#     alpha=0.3,\n",
    "# )\n",
    "\n",
    "\n",
    "# plot vertical lines at test splits\n",
    "for split in train_splits[:-1]:\n",
    "    ax.axvline(x=split, color=\"k\", linestyle=\"--\", linewidth=1)\n",
    "\n",
    "# Setting labels, title, and legend\n",
    "ax.set_xlabel(\"Batch Number\")\n",
    "ax.set_ylabel(\"Train Accuracy\")\n",
    "ax.set_title(\n",
    "    \"ProbaSlope Delegation Ensemble vs Full Ensemble vs Restricted Max Guru Delegation Ensemble\"\n",
    ")\n",
    "\n",
    "ax.legend(loc=\"upper left\")\n",
    "# set y lim to lower\n",
    "ax.set_ylim(top=1.3)\n",
    "# set y ticks to 0-1\n",
    "ax.set_yticks(np.arange(0, 1.1, 0.1))\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ps_voters = exp.ensembles[1].voters\n",
    "print(ensembles[1].name)\n",
    "batch_accs = []\n",
    "for v in ps_voters:\n",
    "    batch_accs.append(v.batch_accuracies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len_train = len(data.train_data_loader.dataset) / batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_active_streaks(voter_id, trial_num):\n",
    "    \"\"\"\n",
    "    Find active streaks for a specified voter.\n",
    "\n",
    "    :param voter_id: ID of the voter for which to find active streaks.\n",
    "    :param batch_metric_values: Dictionary containing the batch metric values.\n",
    "    :param metric_key: Key to access the relevant metric in batch_metric_values.\n",
    "    :return: List of active streaks for the specified voter.\n",
    "    \"\"\"\n",
    "    active_batches = []\n",
    "    active_streak = [None, None]\n",
    "    voter_active = False\n",
    "\n",
    "    for i, av in enumerate(\n",
    "        exp.batch_metric_values[\"proba_slope_delegations\"][trial_num][\n",
    "            \"active_voters-train\"\n",
    "        ]\n",
    "    ):\n",
    "        # print(av)\n",
    "        if voter_id in av:\n",
    "            if not voter_active:\n",
    "                # Start a new streak\n",
    "                active_streak[0] = i\n",
    "                voter_active = True\n",
    "                # print(\"streak started\")\n",
    "            active_streak[1] = i\n",
    "        else:\n",
    "            if voter_active:\n",
    "                # End the current streak\n",
    "                active_batches.append(active_streak.copy())\n",
    "                active_streak = [None, None]\n",
    "                voter_active = False\n",
    "                # print(\"streak done\")\n",
    "\n",
    "    # Handle case where the streak continues till the end of the list\n",
    "    if voter_active:\n",
    "        active_batches.append(active_streak.copy())\n",
    "\n",
    "    return active_batches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Look at activity on last trial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for voter_id in range(n_voters):\n",
    "    active_streaks = find_active_streaks(voter_id, num_trials - 1)\n",
    "    # print(f\"Active Streaks for Voter {voter_id}: {active_streaks}\")\n",
    "\n",
    "    plt.figure(figsize=(10, 5))  # Create a new figure for each voter\n",
    "    plt.plot(batch_accs[voter_id])\n",
    "    plt.axvline(x=len_train, color=\"k\", linestyle=\"--\", linewidth=1)\n",
    "\n",
    "    # Shade the active batches for this voter\n",
    "    for streak in active_streaks:\n",
    "        if streak[0] is not None and streak[1] is not None:\n",
    "            plt.axvspan(streak[0], streak[1], alpha=0.3, color=\"red\")\n",
    "\n",
    "    # Plot a green vertical line at all train splits\n",
    "    for split in train_splits[:-1]:\n",
    "        plt.axvline(x=split, color=\"g\", linestyle=\"--\", linewidth=2)\n",
    "\n",
    "    plt.title(f\"Voter {voter_id} Activity\")\n",
    "    plt.xlabel(\"Batches\")\n",
    "    plt.ylabel(\"Accuracy\")\n",
    "    plt.show()  # Display the plot for each voter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "LDE",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
