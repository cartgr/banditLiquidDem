{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "# import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# from scipy import stats\n",
    "from tqdm import tqdm\n",
    "\n",
    "# from matplotlib.animation import FuncAnimation\n",
    "from deslib.des.meta_des import METADES\n",
    "from sklearn.model_selection import train_test_split\n",
    "from skorch import NeuralNetClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3.5\n"
     ]
    }
   ],
   "source": [
    "# print the version of DESLIB\n",
    "import deslib\n",
    "\n",
    "print(deslib.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Define the transformation and load the MNIST dataset\n",
    "transform = transforms.Compose(\n",
    "    [transforms.ToTensor(), transforms.Normalize((0.5,), (0.5,))]\n",
    ")\n",
    "\n",
    "# Load MNIST dataset\n",
    "mnist_train = datasets.MNIST(\n",
    "    root=\"./data\", train=True, download=True, transform=transform\n",
    ")\n",
    "mnist_test = datasets.MNIST(\n",
    "    root=\"./data\", train=False, download=True, transform=transform\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.fc1 = nn.Linear(28 * 28, 256)\n",
    "        self.fc2 = nn.Linear(256, 64)\n",
    "        self.fc3 = nn.Linear(64, 10)  # 10 output classes for MNIST digits\n",
    "\n",
    "    def forward(self, x):\n",
    "        if x.dim() > 2:\n",
    "            x = x.view(-1, 28 * 28)  # Flatten the image\n",
    "        x = x.float()  # Convert to float\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        return self.fc3(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare the data loaders\n",
    "def prepare_loaders(batch_size=64):\n",
    "    # Get indices of training samples based on the label\n",
    "    train_indices_01 = [\n",
    "        i for i, label in enumerate(mnist_train.targets) if label == 0 or label == 1\n",
    "    ]\n",
    "    train_indices_23 = [\n",
    "        i for i, label in enumerate(mnist_train.targets) if label == 2 or label == 3\n",
    "    ]\n",
    "    train_indices_45 = [\n",
    "        i for i, label in enumerate(mnist_train.targets) if label == 4 or label == 5\n",
    "    ]\n",
    "    train_indices_67 = [\n",
    "        i for i, label in enumerate(mnist_train.targets) if label == 6 or label == 7\n",
    "    ]\n",
    "    train_indices_89 = [\n",
    "        i for i, label in enumerate(mnist_train.targets) if label == 8 or label == 9\n",
    "    ]\n",
    "\n",
    "    # Create Subsets\n",
    "    subset_train_01 = Subset(mnist_train, train_indices_01)\n",
    "    subset_train_23 = Subset(mnist_train, train_indices_23)\n",
    "    subset_train_45 = Subset(mnist_train, train_indices_45)\n",
    "    subset_train_67 = Subset(mnist_train, train_indices_67)\n",
    "    subset_train_89 = Subset(mnist_train, train_indices_89)\n",
    "\n",
    "    # Create DataLoader for all subsets\n",
    "    loader_train_01 = DataLoader(subset_train_01, batch_size=64, shuffle=True)\n",
    "    loader_train_23 = DataLoader(subset_train_23, batch_size=64, shuffle=True)\n",
    "    loader_train_45 = DataLoader(subset_train_45, batch_size=64, shuffle=True)\n",
    "    loader_train_67 = DataLoader(subset_train_67, batch_size=64, shuffle=True)\n",
    "    loader_train_89 = DataLoader(subset_train_89, batch_size=64, shuffle=True)\n",
    "\n",
    "    return (\n",
    "        loader_train_01,\n",
    "        loader_train_23,\n",
    "        loader_train_45,\n",
    "        loader_train_67,\n",
    "        loader_train_89,\n",
    "    )\n",
    "\n",
    "\n",
    "def prepare_des_loaders(batch_size=64):\n",
    "    train_indices, val_indices = train_test_split(\n",
    "        list(range(len(mnist_train))), test_size=0.2, random_state=42\n",
    "    )\n",
    "\n",
    "    train_indices_01 = [\n",
    "        i\n",
    "        for i in train_indices\n",
    "        if mnist_train.targets[i] == 0 or mnist_train.targets[i] == 1\n",
    "    ]\n",
    "    train_indices_23 = [\n",
    "        i\n",
    "        for i in train_indices\n",
    "        if mnist_train.targets[i] == 2 or mnist_train.targets[i] == 3\n",
    "    ]\n",
    "    train_indices_45 = [\n",
    "        i\n",
    "        for i in train_indices\n",
    "        if mnist_train.targets[i] == 4 or mnist_train.targets[i] == 5\n",
    "    ]\n",
    "    train_indices_67 = [\n",
    "        i\n",
    "        for i in train_indices\n",
    "        if mnist_train.targets[i] == 6 or mnist_train.targets[i] == 7\n",
    "    ]\n",
    "    train_indices_89 = [\n",
    "        i\n",
    "        for i in train_indices\n",
    "        if mnist_train.targets[i] == 8 or mnist_train.targets[i] == 9\n",
    "    ]\n",
    "\n",
    "\n",
    "    subset_train_01 = Subset(mnist_train, train_indices_01)\n",
    "    subset_train_23 = Subset(mnist_train, train_indices_23)\n",
    "    subset_train_45 = Subset(mnist_train, train_indices_45)\n",
    "    subset_train_67 = Subset(mnist_train, train_indices_67)\n",
    "    subset_train_89 = Subset(mnist_train, train_indices_89)\n",
    "\n",
    "    subset_val = Subset(mnist_train, val_indices)\n",
    "\n",
    "    loader_train_01 = DataLoader(subset_train_01, batch_size=64, shuffle=True)\n",
    "    loader_train_23 = DataLoader(subset_train_23, batch_size=64, shuffle=True)\n",
    "    loader_train_45 = DataLoader(subset_train_45, batch_size=64, shuffle=True)\n",
    "    loader_train_67 = DataLoader(subset_train_67, batch_size=64, shuffle=True)\n",
    "    loader_train_89 = DataLoader(subset_train_89, batch_size=64, shuffle=True)\n",
    "\n",
    "    loader_val = DataLoader(subset_val, batch_size=64, shuffle=True)\n",
    "\n",
    "    return (\n",
    "        loader_train_01,\n",
    "        loader_train_23,\n",
    "        loader_train_45,\n",
    "        loader_train_67,\n",
    "        loader_train_89,\n",
    "        loader_val,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "loader_01, loader_23, loader_45, loader_67, loader_89 = prepare_loaders()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "(\n",
    "    loader_train_01,\n",
    "    loader_train_23,\n",
    "    loader_train_45,\n",
    "    loader_train_67,\n",
    "    loader_train_89,\n",
    "    loader_val,\n",
    ") = prepare_des_loaders()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loader = DataLoader(mnist_test, batch_size=64, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_single_model(model, loaders, optimizer, criterion, epochs=5):\n",
    "    model.train()\n",
    "    for loader in tqdm(loaders):\n",
    "        for epoch in range(epochs):\n",
    "            for batch_idx, (data, target) in enumerate(loader):\n",
    "                data, target = data.to(device), target.to(device)\n",
    "                optimizer.zero_grad()\n",
    "                output = model(data)\n",
    "                loss = criterion(output, target)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "\n",
    "def train_skorch_model(model, loaders):\n",
    "    for loader in loaders:\n",
    "        dataset = loader.dataset.dataset\n",
    "        indices = loader.dataset.indices\n",
    "\n",
    "        # Ensure X is a float\n",
    "        X = dataset.data[indices].float()\n",
    "\n",
    "        # Ensure y is a long integer\n",
    "        y = dataset.targets[indices].long()\n",
    "\n",
    "        # partial_fit\n",
    "\n",
    "        model.partial_fit(X, y)\n",
    "\n",
    "\n",
    "def test_model(model, loader):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for data, target in loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = model(data)\n",
    "            pred = output.argmax(dim=1, keepdim=True)\n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "    return correct / len(loader.dataset)\n",
    "\n",
    "\n",
    "def train_des_ensemble(models, loader):\n",
    "    original_dataset = loader.dataset.dataset\n",
    "    indices = loader.dataset.indices\n",
    "\n",
    "    # Reshape X to be 2-dimensional and convert to float\n",
    "    X = original_dataset.data[indices].float()\n",
    "    X = X.view(X.size(0), -1)  # Flatten the images into vectors\n",
    "\n",
    "    # Ensure y is a long integer\n",
    "    y = original_dataset.targets[indices].long()\n",
    "\n",
    "    # Initialize and fit the METADES ensemble\n",
    "    des = METADES(models)\n",
    "    des.fit(X, y.long())  # y is also converted to long if necessary\n",
    "\n",
    "    return des"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [00:14<00:00,  2.96s/it]\n"
     ]
    }
   ],
   "source": [
    "# Define the model, optimizer and loss function\n",
    "model = Net().to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Train the model\n",
    "train_single_model(\n",
    "    model, [loader_01, loader_23, loader_45, loader_67, loader_89], optimizer, criterion\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.1918"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test the model on the test set\n",
    "test_model(model, test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_loss     dur\n",
      "-------  ------------  ------\n",
      "      1        \u001b[36m0.1052\u001b[0m  0.1535\n",
      "      2        \u001b[36m0.0255\u001b[0m  0.1467\n",
      "      3        \u001b[36m0.0097\u001b[0m  0.1786\n",
      "      4        \u001b[36m0.0009\u001b[0m  0.1575\n",
      "      5        \u001b[36m0.0000\u001b[0m  0.2042\n",
      "      6        1.0355  0.1531\n",
      "      7        0.0560  0.1499\n",
      "      8        0.0322  0.1495\n",
      "      9        0.0243  0.1431\n",
      "     10        0.0266  0.1498\n",
      "     11        0.5614  0.1393\n",
      "     12        0.0180  0.1309\n",
      "     13        0.0100  0.1366\n",
      "     14        0.0049  0.1370\n",
      "     15        0.0015  0.1329\n",
      "     16        0.4538  0.1447\n",
      "     17        0.0039  0.1497\n",
      "     18        0.0022  0.1434\n",
      "     19        0.0009  0.1435\n",
      "     20        0.0004  0.1419\n",
      "     21        3.8988  0.1373\n",
      "     22        1.5740  0.1366\n",
      "     23        1.3408  0.1375\n",
      "     24        1.1938  0.1358\n",
      "     25        1.0934  0.1346\n",
      "  epoch    train_loss     dur\n",
      "-------  ------------  ------\n",
      "      1        \u001b[36m0.1625\u001b[0m  0.1513\n",
      "      2        \u001b[36m0.0206\u001b[0m  0.1502\n",
      "      3        \u001b[36m0.0166\u001b[0m  0.1496\n",
      "      4        \u001b[36m0.0013\u001b[0m  0.1500\n",
      "      5        \u001b[36m0.0000\u001b[0m  0.1486\n",
      "      6        0.9724  0.1416\n",
      "      7        0.0643  0.1426\n",
      "      8        0.0334  0.1416\n",
      "      9        0.0222  0.1444\n",
      "     10        0.0153  0.1508\n",
      "     11        0.6599  0.1426\n",
      "     12        0.0211  0.1430\n",
      "     13        0.0115  0.1416\n",
      "     14        0.0068  0.1375\n",
      "     15        0.0041  0.1364\n",
      "     16        0.8291  0.1564\n",
      "     17        0.0055  0.1555\n",
      "     18        0.0018  0.1471\n",
      "     19        0.0009  0.1480\n",
      "     20        0.0005  0.1558\n",
      "     21        2.3392  0.1510\n",
      "     22        0.0415  0.1451\n",
      "     23        0.0318  0.1436\n",
      "     24        0.0272  0.1492\n",
      "     25        0.0228  0.1493\n",
      "  epoch    train_loss     dur\n",
      "-------  ------------  ------\n",
      "      1        \u001b[36m0.0859\u001b[0m  0.1624\n",
      "      2        \u001b[36m0.0346\u001b[0m  0.1615\n",
      "      3        \u001b[36m0.0079\u001b[0m  0.1617\n",
      "      4        \u001b[36m0.0034\u001b[0m  0.1652\n",
      "      5        \u001b[36m0.0007\u001b[0m  0.1635\n",
      "      6        0.7692  0.1544\n",
      "      7        0.0498  0.1499\n",
      "      8        0.0304  0.1505\n",
      "      9        0.0229  0.1502\n",
      "     10        0.0181  0.1539\n",
      "     11        0.8175  0.1354\n",
      "     12        0.0201  0.1355\n",
      "     13        0.0119  0.1341\n",
      "     14        0.0104  0.1354\n",
      "     15        0.0060  0.1343\n",
      "     16        2.9838  0.1448\n",
      "     17        1.5750  0.1444\n",
      "     18        1.3426  0.1460\n",
      "     19        1.1952  0.1463\n",
      "     20        1.0940  0.1471\n",
      "     21        3.1967  0.1714\n",
      "     22        2.6711  0.1456\n",
      "     23        2.3077  0.1438\n",
      "     24        2.0248  0.1422\n",
      "     25        1.7982  0.1398\n",
      "  epoch    train_loss     dur\n",
      "-------  ------------  ------\n",
      "      1        \u001b[36m0.1014\u001b[0m  0.1536\n",
      "      2        \u001b[36m0.0172\u001b[0m  0.1522\n",
      "      3        \u001b[36m0.0051\u001b[0m  0.1514\n",
      "      4        0.0251  0.1519\n",
      "      5        0.0137  0.1516\n",
      "      6        0.6279  0.1459\n",
      "      7        0.0344  0.1451\n",
      "      8        0.0221  0.1469\n",
      "      9        0.0197  0.1451\n",
      "     10        0.0130  0.1459\n",
      "     11        0.4561  0.1361\n",
      "     12        0.0172  0.1284\n",
      "     13        0.0077  0.1345\n",
      "     14        \u001b[36m0.0031\u001b[0m  0.1341\n",
      "     15        \u001b[36m0.0022\u001b[0m  0.1343\n",
      "     16        0.4184  0.1461\n",
      "     17        0.0046  0.1522\n",
      "     18        \u001b[36m0.0018\u001b[0m  0.1448\n",
      "     19        \u001b[36m0.0013\u001b[0m  0.1453\n",
      "     20        0.0017  0.1468\n",
      "     21        1.8287  0.1428\n",
      "     22        0.7939  0.1421\n",
      "     23        0.6122  0.1429\n",
      "     24        0.4919  0.1417\n",
      "     25        0.3945  0.1410\n",
      "  epoch    train_loss     dur\n",
      "-------  ------------  ------\n",
      "      1        \u001b[36m0.1012\u001b[0m  0.1502\n",
      "      2        \u001b[36m0.0153\u001b[0m  0.1495\n",
      "      3        0.0178  0.1485\n",
      "      4        \u001b[36m0.0053\u001b[0m  0.1701\n",
      "      5        0.0142  0.1472\n",
      "      6        0.6025  0.1425\n",
      "      7        0.0643  0.1422\n",
      "      8        0.0324  0.1419\n",
      "      9        0.0207  0.1506\n",
      "     10        0.0181  0.1509\n",
      "     11        0.4216  0.1415\n",
      "     12        0.0151  0.1314\n",
      "     13        0.0094  0.1348\n",
      "     14        \u001b[36m0.0049\u001b[0m  0.1329\n",
      "     15        \u001b[36m0.0028\u001b[0m  0.1332\n",
      "     16        0.7432  0.1421\n",
      "     17        0.0041  0.1424\n",
      "     18        \u001b[36m0.0013\u001b[0m  0.1419\n",
      "     19        \u001b[36m0.0003\u001b[0m  0.1403\n",
      "     20        0.0006  0.1413\n",
      "     21        1.1835  0.1456\n",
      "     22        0.0485  0.1417\n",
      "     23        0.0380  0.1388\n",
      "     24        0.0285  0.1385\n",
      "     25        0.0223  0.1379\n",
      "  epoch    train_loss     dur\n",
      "-------  ------------  ------\n",
      "      1        \u001b[36m0.0946\u001b[0m  0.1591\n",
      "      2        \u001b[36m0.0221\u001b[0m  0.1974\n",
      "      3        \u001b[36m0.0074\u001b[0m  0.1535\n",
      "      4        \u001b[36m0.0034\u001b[0m  0.1528\n",
      "      5        \u001b[36m0.0028\u001b[0m  0.1548\n",
      "      6        0.5410  0.1490\n",
      "      7        0.0423  0.1426\n",
      "      8        0.0297  0.1503\n",
      "      9        0.0213  0.1548\n",
      "     10        0.0121  0.1545\n",
      "     11        0.4707  0.1489\n",
      "     12        0.0137  0.1424\n",
      "     13        0.0086  0.1441\n",
      "     14        0.0061  0.1779\n",
      "     15        0.0043  0.1653\n",
      "     16        0.9024  0.1518\n",
      "     17        0.0058  0.1419\n",
      "     18        0.0053  0.1407\n",
      "     19        \u001b[36m0.0018\u001b[0m  0.1435\n",
      "     20        \u001b[36m0.0006\u001b[0m  0.1592\n",
      "     21        3.8140  0.1500\n",
      "     22        1.6186  0.1367\n",
      "     23        1.3736  0.1356\n",
      "     24        1.2186  0.1376\n",
      "     25        1.1125  0.1520\n",
      "  epoch    train_loss     dur\n",
      "-------  ------------  ------\n",
      "      1        \u001b[36m0.1316\u001b[0m  0.1495\n",
      "      2        \u001b[36m0.0177\u001b[0m  0.1605\n",
      "      3        \u001b[36m0.0100\u001b[0m  0.1595\n",
      "      4        \u001b[36m0.0053\u001b[0m  0.1516\n",
      "      5        \u001b[36m0.0005\u001b[0m  0.1504\n",
      "      6        0.6353  0.1449\n",
      "      7        0.0451  0.1434\n",
      "      8        0.0262  0.1418\n",
      "      9        0.0242  0.1476\n",
      "     10        0.0205  0.1428\n",
      "     11        0.5020  0.1350\n",
      "     12        0.0170  0.1346\n",
      "     13        0.0064  0.1321\n",
      "     14        0.0050  0.1421\n",
      "     15        0.0047  0.1360\n",
      "     16        2.5855  0.1828\n",
      "     17        1.5237  0.1586\n",
      "     18        1.2986  0.1660\n",
      "     19        1.1588  0.1519\n",
      "     20        1.0641  0.1584\n",
      "     21        3.2128  0.1567\n",
      "     22        2.6767  0.1442\n",
      "     23        2.3074  0.1398\n",
      "     24        2.0209  0.1392\n",
      "     25        1.7923  0.1382\n",
      "  epoch    train_loss     dur\n",
      "-------  ------------  ------\n",
      "      1        \u001b[36m0.1450\u001b[0m  0.1483\n",
      "      2        \u001b[36m0.0453\u001b[0m  0.1482\n",
      "      3        \u001b[36m0.0054\u001b[0m  0.1493\n",
      "      4        \u001b[36m0.0001\u001b[0m  0.1477\n",
      "      5        0.0057  0.1468\n",
      "      6        0.9023  0.1425\n",
      "      7        0.0650  0.1418\n",
      "      8        0.0333  0.1405\n",
      "      9        0.0294  0.1420\n",
      "     10        0.0223  0.1418\n",
      "     11        0.8950  0.1286\n",
      "     12        0.0127  0.1289\n",
      "     13        0.0053  0.1298\n",
      "     14        0.0026  0.1309\n",
      "     15        0.0015  0.1292\n",
      "     16        1.5833  0.1403\n",
      "     17        0.0090  0.1392\n",
      "     18        0.0033  0.1418\n",
      "     19        0.0005  0.1396\n",
      "     20        0.0001  0.1425\n",
      "     21        2.3898  0.1384\n",
      "     22        0.0480  0.1414\n",
      "     23        0.0322  0.1390\n",
      "     24        0.0245  0.1401\n",
      "     25        0.0196  0.1390\n",
      "  epoch    train_loss     dur\n",
      "-------  ------------  ------\n",
      "      1        \u001b[36m0.1233\u001b[0m  0.1477\n",
      "      2        \u001b[36m0.0512\u001b[0m  0.1495\n",
      "      3        \u001b[36m0.0160\u001b[0m  0.1483\n",
      "      4        \u001b[36m0.0086\u001b[0m  0.1489\n",
      "      5        \u001b[36m0.0068\u001b[0m  0.1479\n",
      "      6        0.5670  0.1414\n",
      "      7        0.0354  0.1403\n",
      "      8        0.0271  0.1461\n",
      "      9        0.0193  0.1416\n",
      "     10        0.0093  0.1412\n",
      "     11        0.5372  0.1311\n",
      "     12        0.0130  0.1309\n",
      "     13        \u001b[36m0.0068\u001b[0m  0.1321\n",
      "     14        \u001b[36m0.0051\u001b[0m  0.1324\n",
      "     15        \u001b[36m0.0036\u001b[0m  0.1302\n",
      "     16        0.8395  0.1417\n",
      "     17        0.0037  0.1401\n",
      "     18        \u001b[36m0.0020\u001b[0m  0.1418\n",
      "     19        \u001b[36m0.0013\u001b[0m  0.1413\n",
      "     20        \u001b[36m0.0007\u001b[0m  0.1422\n",
      "     21        1.5353  0.1382\n",
      "     22        0.0431  0.1372\n",
      "     23        0.0280  0.1362\n",
      "     24        0.0201  0.1374\n",
      "     25        0.0162  0.1380\n",
      "  epoch    train_loss     dur\n",
      "-------  ------------  ------\n",
      "      1        \u001b[36m1.8220\u001b[0m  0.1482\n",
      "      2        \u001b[36m0.0449\u001b[0m  0.1481\n",
      "      3        \u001b[36m0.0257\u001b[0m  0.1453\n",
      "      4        \u001b[36m0.0036\u001b[0m  0.1473\n",
      "      5        0.0061  0.1490\n",
      "      6        3.1292  0.1414\n",
      "      7        0.7313  0.1430\n",
      "      8        0.7147  0.1403\n",
      "      9        0.7080  0.1413\n",
      "     10        0.7044  0.1409\n",
      "     11        2.8070  0.1317\n",
      "     12        0.8346  0.1314\n",
      "     13        0.7548  0.1300\n",
      "     14        0.7316  0.1317\n",
      "     15        0.7205  0.1310\n",
      "     16        3.3007  0.1408\n",
      "     17        0.8814  0.1423\n",
      "     18        0.7693  0.1422\n",
      "     19        0.7397  0.1412\n",
      "     20        0.7260  0.1413\n",
      "     21        3.6488  0.1387\n",
      "     22        0.9221  0.1364\n",
      "     23        0.7819  0.1391\n",
      "     24        0.7471  0.1385\n",
      "     25        0.7312  0.1381\n",
      "  epoch    train_loss     dur\n",
      "-------  ------------  ------\n",
      "      1   \u001b[36m145491.1789\u001b[0m  0.1482\n",
      "      2        \u001b[36m0.4393\u001b[0m  0.1472\n",
      "      3        \u001b[36m0.3771\u001b[0m  0.1486\n",
      "      4        0.3986  0.1477\n",
      "      5        0.5670  0.1478\n",
      "      6     1854.7241  0.1626\n",
      "      7        0.7274  0.1416\n",
      "      8        0.7213  0.1396\n",
      "      9        0.7153  0.1414\n",
      "     10        0.7133  0.1531\n",
      "     11     4653.8052  0.1383\n",
      "     12       15.7872  0.1309\n",
      "     13        0.7455  0.1294\n",
      "     14        0.7362  0.1310\n",
      "     15        0.7261  0.1320\n",
      "     16     3028.4826  0.1395\n",
      "     17        0.7138  0.1414\n",
      "     18        0.7133  0.1418\n",
      "     19        0.7129  0.1408\n",
      "     20        0.7127  0.1423\n",
      "     21        4.4139  0.1387\n",
      "     22        0.7085  0.1358\n",
      "     23        0.7084  0.1374\n",
      "     24        0.7084  0.1366\n",
      "     25        0.7084  0.1385\n"
     ]
    }
   ],
   "source": [
    "# define 5 classifiers for the DES ensemble\n",
    "model_1 = NeuralNetClassifier(\n",
    "    Net,\n",
    "    max_epochs=5,\n",
    "    criterion=nn.CrossEntropyLoss,\n",
    "    optimizer=optim.Adam,\n",
    "    lr=0.001,\n",
    "    device=device,\n",
    "    iterator_train__shuffle=False,\n",
    "    batch_size=64,\n",
    "    train_split=None,\n",
    ")\n",
    "model_2 = NeuralNetClassifier(\n",
    "    Net,\n",
    "    max_epochs=5,\n",
    "    criterion=nn.CrossEntropyLoss,\n",
    "    optimizer=optim.Adam,\n",
    "    lr=0.001,\n",
    "    device=device,\n",
    "    iterator_train__shuffle=False,\n",
    "    batch_size=64,\n",
    "    train_split=None,\n",
    ")\n",
    "\n",
    "model_3 = NeuralNetClassifier(\n",
    "    Net,\n",
    "    max_epochs=5,\n",
    "    criterion=nn.CrossEntropyLoss,\n",
    "    optimizer=optim.Adam,\n",
    "    lr=0.001,\n",
    "    device=device,\n",
    "    iterator_train__shuffle=False,\n",
    "    batch_size=64,\n",
    "    train_split=None,\n",
    ")\n",
    "model_4 = NeuralNetClassifier(\n",
    "    Net,\n",
    "    max_epochs=5,\n",
    "    criterion=nn.CrossEntropyLoss,\n",
    "    optimizer=optim.Adam,\n",
    "    lr=0.001,\n",
    "    device=device,\n",
    "    iterator_train__shuffle=False,\n",
    "    batch_size=64,\n",
    "    train_split=None,\n",
    ")\n",
    "model_5 = NeuralNetClassifier(\n",
    "    Net,\n",
    "    max_epochs=5,\n",
    "    criterion=nn.CrossEntropyLoss,\n",
    "    optimizer=optim.Adam,\n",
    "    lr=0.001,\n",
    "    device=device,\n",
    "    iterator_train__shuffle=False,\n",
    "    batch_size=64,\n",
    "    train_split=None,\n",
    ")\n",
    "\n",
    "model_6 = NeuralNetClassifier(\n",
    "    Net,\n",
    "    max_epochs=5,\n",
    "    criterion=nn.CrossEntropyLoss,\n",
    "    optimizer=optim.Adam,\n",
    "    lr=0.001,\n",
    "    device=device,\n",
    "    iterator_train__shuffle=False,\n",
    "    batch_size=64,\n",
    "    train_split=None,\n",
    ")\n",
    "\n",
    "model_7 = NeuralNetClassifier(\n",
    "    Net,\n",
    "    max_epochs=5,\n",
    "    criterion=nn.CrossEntropyLoss,\n",
    "    optimizer=optim.Adam,\n",
    "    lr=0.001,\n",
    "    device=device,\n",
    "    iterator_train__shuffle=False,\n",
    "    batch_size=64,\n",
    "    train_split=None,\n",
    ")\n",
    "\n",
    "model_8 = NeuralNetClassifier(\n",
    "    Net,\n",
    "    max_epochs=5,\n",
    "    criterion=nn.CrossEntropyLoss,\n",
    "    optimizer=optim.Adam,\n",
    "    lr=0.001,\n",
    "    device=device,\n",
    "    iterator_train__shuffle=False,\n",
    "    batch_size=64,\n",
    "    train_split=None,\n",
    ")\n",
    "\n",
    "model_9 = NeuralNetClassifier(\n",
    "    Net,\n",
    "    max_epochs=5,\n",
    "    criterion=nn.CrossEntropyLoss,\n",
    "    optimizer=optim.Adam,\n",
    "    lr=0.001,\n",
    "    device=device,\n",
    "    iterator_train__shuffle=False,\n",
    "    batch_size=64,\n",
    "    train_split=None,\n",
    ")\n",
    "\n",
    "model_10 = NeuralNetClassifier(\n",
    "    Net,\n",
    "    max_epochs=5,\n",
    "    criterion=nn.CrossEntropyLoss,\n",
    "    optimizer=optim.Adam,\n",
    "    lr=0.01,\n",
    "    device=device,\n",
    "    iterator_train__shuffle=False,\n",
    "    batch_size=64,\n",
    "    train_split=None,\n",
    ")\n",
    "\n",
    "model_11 = NeuralNetClassifier(\n",
    "    Net,\n",
    "    max_epochs=5,\n",
    "    criterion=nn.CrossEntropyLoss,\n",
    "    optimizer=optim.Adam,\n",
    "    lr=1,\n",
    "    device=device,\n",
    "    iterator_train__shuffle=False,\n",
    "    batch_size=64,\n",
    "    train_split=None,\n",
    ")\n",
    "\n",
    "models = [\n",
    "    model_1,\n",
    "    model_2,\n",
    "    model_3,\n",
    "    model_4,\n",
    "    model_5,\n",
    "    model_6,\n",
    "    model_7,\n",
    "    model_8,\n",
    "    model_9,\n",
    "    model_10,\n",
    "    model_11,\n",
    "]\n",
    "\n",
    "# train the models\n",
    "for model in models:\n",
    "    train_skorch_model(\n",
    "        model,\n",
    "        [\n",
    "            loader_train_01,\n",
    "            loader_train_23,\n",
    "            loader_train_45,\n",
    "            loader_train_67,\n",
    "            loader_train_89,\n",
    "        ],\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of model_1:  0.1009\n",
      "Accuracy of model_2:  0.1952\n",
      "Accuracy of model_3:  0.1028\n",
      "Accuracy of model_4:  0.1953\n",
      "Accuracy of model_5:  0.1945\n",
      "Accuracy of model_6:  0.1009\n",
      "Accuracy of model_7:  0.1028\n",
      "Accuracy of model_8:  0.1959\n",
      "Accuracy of model_9:  0.1947\n",
      "Accuracy of model_10:  0.0974\n"
     ]
    }
   ],
   "source": [
    "# predict on the test set with model_1\n",
    "y_pred = model_1.predict(test_loader.dataset.data.float())\n",
    "\n",
    "accuracy = np.sum(y_pred == test_loader.dataset.targets.numpy()) / len(\n",
    "    test_loader.dataset.targets.numpy()\n",
    ")\n",
    "print(\"Accuracy of model_1: \", accuracy)\n",
    "\n",
    "y_pred = model_2.predict(test_loader.dataset.data.float())\n",
    "\n",
    "accuracy = np.sum(y_pred == test_loader.dataset.targets.numpy()) / len(\n",
    "    test_loader.dataset.targets.numpy()\n",
    ")\n",
    "print(\"Accuracy of model_2: \", accuracy)\n",
    "\n",
    "y_pred = model_3.predict(test_loader.dataset.data.float())\n",
    "\n",
    "accuracy = np.sum(y_pred == test_loader.dataset.targets.numpy()) / len(\n",
    "    test_loader.dataset.targets.numpy()\n",
    ")\n",
    "print(\"Accuracy of model_3: \", accuracy)\n",
    "\n",
    "y_pred = model_4.predict(test_loader.dataset.data.float())\n",
    "\n",
    "accuracy = np.sum(y_pred == test_loader.dataset.targets.numpy()) / len(\n",
    "    test_loader.dataset.targets.numpy()\n",
    ")\n",
    "print(\"Accuracy of model_4: \", accuracy)\n",
    "\n",
    "y_pred = model_5.predict(test_loader.dataset.data.float())\n",
    "\n",
    "accuracy = np.sum(y_pred == test_loader.dataset.targets.numpy()) / len(\n",
    "    test_loader.dataset.targets.numpy()\n",
    ")\n",
    "print(\"Accuracy of model_5: \", accuracy)\n",
    "\n",
    "y_pred = model_6.predict(test_loader.dataset.data.float())\n",
    "\n",
    "accuracy = np.sum(y_pred == test_loader.dataset.targets.numpy()) / len(\n",
    "    test_loader.dataset.targets.numpy()\n",
    ")\n",
    "print(\"Accuracy of model_6: \", accuracy)\n",
    "\n",
    "y_pred = model_7.predict(test_loader.dataset.data.float())\n",
    "\n",
    "accuracy = np.sum(y_pred == test_loader.dataset.targets.numpy()) / len(\n",
    "    test_loader.dataset.targets.numpy()\n",
    ")\n",
    "print(\"Accuracy of model_7: \", accuracy)\n",
    "\n",
    "y_pred = model_8.predict(test_loader.dataset.data.float())\n",
    "\n",
    "accuracy = np.sum(y_pred == test_loader.dataset.targets.numpy()) / len(\n",
    "    test_loader.dataset.targets.numpy()\n",
    ")\n",
    "print(\"Accuracy of model_8: \", accuracy)\n",
    "\n",
    "y_pred = model_9.predict(test_loader.dataset.data.float())\n",
    "\n",
    "accuracy = np.sum(y_pred == test_loader.dataset.targets.numpy()) / len(\n",
    "    test_loader.dataset.targets.numpy()\n",
    ")\n",
    "\n",
    "print(\"Accuracy of model_9: \", accuracy)\n",
    "\n",
    "y_pred = model_10.predict(test_loader.dataset.data.float())\n",
    "\n",
    "accuracy = np.sum(y_pred == test_loader.dataset.targets.numpy()) / len(\n",
    "    test_loader.dataset.targets.numpy()\n",
    ")\n",
    "\n",
    "print(\"Accuracy of model_10: \", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of vanilla ensemble:  0.1961\n"
     ]
    }
   ],
   "source": [
    "# now lets use them as an ensemble\n",
    "def predict_ensemble(models, loader):\n",
    "    # get the pred_proba for each model and average, then take the argmax\n",
    "    pred_proba = []\n",
    "    for model in models:\n",
    "        pred_proba.append(model.predict_proba(loader.dataset.data.float()))\n",
    "    pred_proba = np.array(pred_proba)\n",
    "    pred_proba = np.mean(pred_proba, axis=0)\n",
    "    return np.argmax(pred_proba, axis=1)\n",
    "\n",
    "\n",
    "y_pred = predict_ensemble(models, test_loader)\n",
    "accuracy = np.sum(y_pred == test_loader.dataset.targets.numpy()) / len(\n",
    "    test_loader.dataset.targets.numpy()\n",
    ")\n",
    "print(\"Accuracy of vanilla ensemble: \", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'numpy' has no attribute 'int'.\n`np.int` was a deprecated alias for the builtin `int`. To avoid this error in existing code, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\nThe aliases was originally deprecated in NumPy 1.20; for more details and guidance see the original release note at:\n    https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[17], line 7\u001b[0m\n\u001b[1;32m      5\u001b[0m X \u001b[38;5;241m=\u001b[39m X\u001b[38;5;241m.\u001b[39mview(X\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m), \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mfloat()  \u001b[38;5;66;03m# Flatten the images into vectors\u001b[39;00m\n\u001b[1;32m      6\u001b[0m y \u001b[38;5;241m=\u001b[39m loader_val\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39mtargets\u001b[38;5;241m.\u001b[39mlong()\n\u001b[0;32m----> 7\u001b[0m \u001b[43mmeta\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/VSCode/banditLiquidDem/.conda/lib/python3.11/site-packages/deslib/des/meta_des.py:211\u001b[0m, in \u001b[0;36mMETADES.fit\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m    205\u001b[0m \u001b[38;5;66;03m# check whether the meta-classifier was already trained since\u001b[39;00m\n\u001b[1;32m    206\u001b[0m \u001b[38;5;66;03m# it could have been pre-processed before\u001b[39;00m\n\u001b[1;32m    207\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmeta_classifier_, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mclasses_\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m    208\u001b[0m \n\u001b[1;32m    209\u001b[0m     \u001b[38;5;66;03m# IF it is not fitted, generate the meta-training dataset and\u001b[39;00m\n\u001b[1;32m    210\u001b[0m     \u001b[38;5;66;03m# train the meta-classifier\u001b[39;00m\n\u001b[0;32m--> 211\u001b[0m     X_meta, y_meta \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_generate_meta_training_set\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    212\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fit_meta_classifier(X_meta, y_meta)\n\u001b[1;32m    214\u001b[0m \u001b[38;5;66;03m# set the number of meta-features\u001b[39;00m\n",
      "File \u001b[0;32m~/VSCode/banditLiquidDem/.conda/lib/python3.11/site-packages/deslib/des/meta_des.py:351\u001b[0m, in \u001b[0;36mMETADES._generate_meta_training_set\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    349\u001b[0m meta_feature_target \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mDSEL_processed_[indices_selected, :]\n\u001b[1;32m    350\u001b[0m meta_feature_target \u001b[38;5;241m=\u001b[39m meta_feature_target\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, )\n\u001b[0;32m--> 351\u001b[0m meta_feature_target\u001b[38;5;241m.\u001b[39mastype(\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mint\u001b[49m)\n\u001b[1;32m    353\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m meta_feature_vector, meta_feature_target\n",
      "File \u001b[0;32m~/VSCode/banditLiquidDem/.conda/lib/python3.11/site-packages/numpy/__init__.py:324\u001b[0m, in \u001b[0;36m__getattr__\u001b[0;34m(attr)\u001b[0m\n\u001b[1;32m    319\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[1;32m    320\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIn the future `np.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mattr\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m` will be defined as the \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    321\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcorresponding NumPy scalar.\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;167;01mFutureWarning\u001b[39;00m, stacklevel\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)\n\u001b[1;32m    323\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m attr \u001b[38;5;129;01min\u001b[39;00m __former_attrs__:\n\u001b[0;32m--> 324\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(__former_attrs__[attr])\n\u001b[1;32m    326\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m attr \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtesting\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m    327\u001b[0m     \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtesting\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mtesting\u001b[39;00m\n",
      "\u001b[0;31mAttributeError\u001b[0m: module 'numpy' has no attribute 'int'.\n`np.int` was a deprecated alias for the builtin `int`. To avoid this error in existing code, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\nThe aliases was originally deprecated in NumPy 1.20; for more details and guidance see the original release note at:\n    https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations"
     ]
    }
   ],
   "source": [
    "# now lets try the DES\n",
    "meta = METADES(models)\n",
    "# case to double\n",
    "X = loader_val.dataset.dataset.data.float()\n",
    "X = X.view(X.size(0), -1).float()  # Flatten the images into vectors\n",
    "y = loader_val.dataset.dataset.targets.long()\n",
    "meta.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of DES:  0.2928\n"
     ]
    }
   ],
   "source": [
    "# make predictions\n",
    "X_test = test_loader.dataset.data.float()\n",
    "X_test = X_test.view(X_test.size(0), -1).float()  # Flatten the images into vectors\n",
    "y_pred = meta.predict(X_test)\n",
    "accuracy = np.sum(y_pred == test_loader.dataset.targets.numpy()) / len(\n",
    "    test_loader.dataset.targets.numpy()\n",
    ")\n",
    "print(\"Accuracy of DES: \", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "liquidDem",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
